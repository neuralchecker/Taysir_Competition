{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TAYSIR Baseline for Track 2\n",
    "# Extraction of WA from RNNs and transformers already Trained on a Language Modelling Task\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This is a notebook to let you play around with the Weighted Automata extraction baseline that use the spectral extraction technique.\n",
    "Inputed Neural Net can be LSTM, GRU, SRN or Transformer network, after which it will draw a neat little WA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "## Imports and version verifying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "\n",
    "print(\"Your torch version:\", torch.__version__)\n",
    "print(\"Your mlflow version:\", mlflow.__version__)\n",
    "import sys\n",
    "print(\"Your python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested with:\n",
    "* Torch version: 1.11.0+cu102\n",
    "* MLFlow version: 1.25.1\n",
    "* Python version: 3.8.10 [GCC 9.4.0]\n",
    "\n",
    "Python versions starting at 3.7 are supposed to work (but have not been tested).\n",
    "\n",
    "## Choosing the task\n",
    "\n",
    "First you must select one of the phases/datasets we provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = 2 #always for this track\n",
    "DATASET = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the RNN of the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"models/2.{DATASET}.taysir.model\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of some variables that would be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: #RNN\n",
    "    nb_letters = model.input_size -1\n",
    "    cell_type = model.cell_type\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The type of the recurrent cells is\", cell_type.__name__)\n",
    "except: #Transformer\n",
    "    nb_letters = model.distilbert.config.vocab_size\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The model is a transformer (DistilBertForSequenceClassification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The input data is in the following format :\n",
    "\n",
    "```\n",
    "[Number of sequences] [Alphabet size]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "...\n",
    "[Length of sequence] [List of symbols]\n",
    "```\n",
    "\n",
    "For example the following data :\n",
    "\n",
    "```\n",
    "5 10\n",
    "6 8 6 5 1 6 7 4 9\n",
    "12 8 6 9 4 6 8 2 1 0 6 5 9\n",
    "7 8 9 4 3 0 4 9\n",
    "4 8 0 4 9\n",
    "8 8 1 5 2 6 0 5 3 9\n",
    "```\n",
    "\n",
    "is composed of 5 sequences and have an alphabet size of 10 (so symbols are between 0 and 9) and the first sequence is composed of 6 symbols (8 6 5 1 6 7 4 9), notice that 8 is the start symbol and 9 is the end symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"datasets/2.{DATASET}.taysir.valid.words\"\n",
    "max_len = 0\n",
    "sequences = []\n",
    "with open(file) as f:\n",
    "    f.readline() #Skip first line (number of sequences, alphabet size)\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        seq = line.split(' ')\n",
    "        length = int(seq[0]) #Get length of the sequence\n",
    "        if length > max_len:\n",
    "            max_len = length\n",
    "        seq = [int(i) for i in seq[1:]] #Remove first value (length) and cast to int\n",
    "        sequences.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *sequences* is thus **a list of lists**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of sequences:', len(sequences))\n",
    "print('10 first sequences:')\n",
    "for i in range(10):\n",
    "    print(sequences[i])\n",
    "print(\"Maximal observed length of the sequences:\", max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HewNg6Ew6Jz"
   },
   "source": [
    "# Model extraction\n",
    "## Seeding\n",
    "We are seeding for reproductibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the trained RNN. It is given as a MLFlow model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WA Extraction Baseline\n",
    "This part is the one you need to change to put your own algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm is the spectral extraction described in this paper: https://arxiv.org/abs/2009.13101\n",
    "\n",
    "We are going to fix a number of prefixes (and of suffixes) and then use the model to generate these numbers of elements. The model will then be used to fill the Hankel matrix from which we will create a Weighted Automaton. \n",
    "\n",
    "We fist define two functions that are of great help for **transformer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This two function are for transformer only\"\"\"\n",
    "def make_future_masks(words:torch.Tensor):\n",
    "    masks = (words != 0)\n",
    "    b,l = masks.size()\n",
    "    #x = einops.einsum(masks, masks, \"b i, b j -> b i j\")\n",
    "    x = torch.einsum(\"bi,bj->bij\",masks,masks)\n",
    "    x *= torch.ones(l,l, dtype=torch.bool, device=x.device).tril()\n",
    "    x += torch.eye(l,dtype=torch.bool, device=x.device)\n",
    "    return x.type(torch.int8)\n",
    "\n",
    "def predict_next_symbol_from_prefix(model, prefix):\n",
    "    \"\"\"\n",
    "    given prefix, outputs the prediction probabilities of the next symbols.\n",
    "    Note: In this function, each id in the word is added to 1 before being input to the model,\n",
    "    since ids 0 is used as special tokens. We do not add the start symbol like [CLS] since all words aleady has a specific start symbol at potision 0\n",
    "        0 : padding id\n",
    "    Args:\n",
    "        prefix (list): prefix like [20,13,14]\n",
    "    Returns:\n",
    "        the predicted probabilities of the next ids (1-D ndarray)\n",
    "    \"\"\"\n",
    "    prefix = [ [ a+1 for a in prefix ] ]\n",
    "    prefix = torch.IntTensor(prefix)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        attention_mask = make_future_masks(prefix)\n",
    "        #print(attention_mask)\n",
    "        out = model.forward(prefix, attention_mask=attention_mask)\n",
    "        out = torch.nn.functional.softmax(out.logits[0,-1], dim=0)\n",
    "        return out.detach().numpy()[1:] # the probability for padding id (0) is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the following function that could be useful for other approaches for all types of network. From the model, the oberved maximum length of a sequence, and the number of letters, it generates a sequence following the dsirtibution of next symbols defined by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_word(model, max_len, nb_letters):\n",
    "    \"\"\" A function that uses the LM-RNN or a Transformer to generate a sequence\"\"\"\n",
    "\n",
    "    current_symbol = nb_letters - 2 #start symbol is always that integer\n",
    "    gen_word = [current_symbol]\n",
    "    len_word = 1\n",
    "    current_hidden = None #Initial state is defined that way\n",
    "    with torch.no_grad():\n",
    "        while len_word < max_len and current_symbol != nb_letters - 1: #end symbol is always nb_letters - 1\n",
    "            try: #RNN\n",
    "                current_one_encoded = model.one_hot_encode([current_symbol])\n",
    "                \"\"\"Despite its name, in LM task, this is the function that provides the probability of the next symbol given a prefix\"\"\"\n",
    "                out, current_hidden = model.forward_bin(current_one_encoded, current_hidden)\n",
    "            except: #Transformer\n",
    "                out = predict_next_symbol_from_prefix(model, gen_word)\n",
    "                out = torch.tensor(out)\n",
    "\n",
    "            \"\"\"Sample next letter acording to the model next symbol distribution\"\"\"\n",
    "            current_symbol = torch.multinomial(out, 1).item()\n",
    "            gen_word += [current_symbol]\n",
    "            len_word +=1\n",
    "    \n",
    "    \"\"\"Make sure the last symbol is the end of sequence one\"\"\"\n",
    "    if len_word == max_len and current_symbol!=nb_letters - 1:\n",
    "        gen_word +=[nb_letters - 1]\n",
    "    \n",
    "    return gen_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our algorithm relies on the use of the toolbox scikit-splearn (https://remieyraud.github.io/scikit-splearn/) that can be installed using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scikit-splearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The spectral distillation baseline\n",
    "There are three steps in our baseline algorithms:\n",
    "1. Use the model to generate sequences and store their sets of prefixes and suffixes\n",
    "2. Fill an object called the Hankel matrix (rows are indexed by prefixes, columns by prefixes, and a cell contains the value of the corresponding sequence) using the model\n",
    "3. Extract a WA from the Hankel matrix using the equations of spectral learning algorithm\n",
    "\n",
    "You are referred to this article for more details https://arxiv.org/abs/2009.13101\n",
    "\n",
    "\n",
    "These functions are needed for our algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_all_prefixes(prefixes_set, word):\n",
    "    \"\"\" add all prefixes of a qequence to an existing set of prefixes\"\"\"\n",
    "    for i in range(2, len(word)):\n",
    "        prefixes_set.add(tuple(word[:i]))\n",
    "def add_all_suffixes(suffixes_set, word):\n",
    "    \"\"\" add all suffixes of a sequence to an existing set of suffixes\"\"\"\n",
    "    for i in range(2, len(word)):\n",
    "        suffixes_set.add(tuple(word[i:]))\n",
    "\n",
    "\"\"\" This function is for transfomers only\"\"\"\n",
    "import numpy\n",
    "def predict_next_symbols(model, word):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        whole word (list): a complete sequence as a list of integers\n",
    "    Returns:\n",
    "        the predicted probabilities of the next ids for all prefixes (2-D ndarray)\n",
    "    \"\"\"\n",
    "    word = [ [ a+1 for a in word ] ]\n",
    "    word = torch.IntTensor(word)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        attention_mask = make_future_masks(word)\n",
    "        out = model.forward(word, attention_mask=attention_mask)\n",
    "        out = torch.nn.functional.softmax(out.logits[0], dim=1)\n",
    "        return out.detach().numpy()[:, 1:] #  the probabilities for padding id (0) are removed\n",
    "\n",
    "\n",
    "def generate_basis(model, nb_prefixes, nb_suffixes, max_len, nb_letters):\n",
    "    \"\"\"A function to generate a set of prefixes and suffixes to be fed later to the model to build the Hankel matrix\"\"\"\n",
    "    words = set()\n",
    "    prefixes = set()\n",
    "    suffixes = set()\n",
    "    with torch.no_grad():\n",
    "        while (len(prefixes) < nb_prefixes or len(suffixes) < nb_suffixes):\n",
    "            gen_word = tuple(generate_one_word(model, max_len, nb_letters))\n",
    "            if gen_word not in words:\n",
    "                words.add(gen_word)\n",
    "                if len(prefixes) < nb_prefixes:\n",
    "                    add_all_prefixes(prefixes, list(gen_word))\n",
    "                if len(suffixes) < nb_suffixes:\n",
    "                    add_all_suffixes(suffixes, list(gen_word))\n",
    "                        \n",
    "    # it is better to sort for the Hankel construction and if the lists start by the delimiting symbols\n",
    "    rows = [(nb_letters-2,)] + sorted(list(prefixes), key=lambda t: (len(t), t[0]))\n",
    "    columns = [(nb_letters-1,)] + sorted(list(suffixes), key=lambda t: (len(t), t[0]))\n",
    "    \n",
    "    # need to create the set of all the words this basis implies to ask the RNN\n",
    "    letters = [[]] + [[i] for i in range(nb_letters)]\n",
    "    all_combinations= set()\n",
    "    for letter in letters:\n",
    "            for prefix in rows:\n",
    "                for suffix in columns:\n",
    "                    all_combinations.add(tuple(list(prefix) + letter + list(suffix)))\n",
    "    return rows, columns, list(all_combinations)\n",
    "\n",
    "def get_values(model, all_combinations, nb_letters):\n",
    "    \"\"\" returns a dictionary with all words in all_combinations as keys and corresponding model assigned values\"\"\"\n",
    "    probas = dict()\n",
    "    for word in all_combinations:\n",
    "        try: #RNN\n",
    "            one_hot_word = model.one_hot_encode(list(word))\n",
    "            value = model.predict(one_hot_word)\n",
    "        except: #Transformer\n",
    "            word = list(word)\n",
    "            probs = predict_next_symbols(model, word[:-1])\n",
    "            probas_for_word = [probs[i,a] for i,a in enumerate(word[1:])]\n",
    "            value = numpy.array(probas_for_word).prod()\n",
    "        probas[tuple(word)] = value\n",
    "\n",
    "    return probas\n",
    "\n",
    "def create_hankels(model, prefixes, suffixes, all_combinations, nb_letters):\n",
    "    \"\"\"\n",
    "    Redefinition of hankels(): return the list of matrices needed for extracting WA\n",
    "    :param model: a RNN in pytorch\n",
    "    :param prefixes: the list of prefixes (for rows)\n",
    "    :param suffixes: the list of suffixes (for columns)\n",
    "    :param all_combinaisons: a list of all the words whose value have to be asked to the RNN\n",
    "    :param nb_letters: the number of letters of the problem\n",
    "    \n",
    "    :return: a list of matrices lhankels. lhankels[0] is the Hankel matrice while\n",
    "             lhankel[i] is H_{i-1}: lhankel[i][prefix][suffix]=predict(prefix + [i] + suffix)\n",
    "    \"\"\"\n",
    "    print(\"Computing Hankels...\")\n",
    "    words_probas = get_values(model, all_combinations, nb_letters)\n",
    "    print(\"    Done using the model\")\n",
    "    \n",
    "    lhankels = [np.zeros((len(prefixes), len(suffixes))) for _ in range(nb_letters+1)]\n",
    "    # empty string and letters matrices:\n",
    "    letters = [[]] + [[i] for i in range(nb_letters)]\n",
    "    for letter in range(len(letters)):\n",
    "        for l in range(len(prefixes)):\n",
    "            for c in range(len(suffixes)):\n",
    "                p = words_probas[prefixes[l] + tuple(letters[letter]) + suffixes[c]]\n",
    "                lhankels[letter][l][c] = p\n",
    "    print(\"    Done computing Hankels\")\n",
    "    return lhankels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can define our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splearn as sp\n",
    "from numpy.linalg import svd, pinv\n",
    "def spectral_distillation(model, nb_states, nb_prefixes, nb_suffixes, max_len, nb_letters):\n",
    "    \"\"\"\n",
    "        Extract a WA of given rank from a RNN\n",
    "        \n",
    "        :param model: a pytorch recurrent model\n",
    "        :param nb_states: the rank of the WA to be extracted\n",
    "        :param nb_prefixes: minimal number of prefixes for the basis \n",
    "        :param nb_suffixes: minimal number of suffixes for the basis \n",
    "        :param max_len: the maximal size of the sequences to generate \n",
    "        :param nb_letters: the number of different symbols \n",
    "        \n",
    "        :results: return the distiled weighted automata  \n",
    "    \"\"\"\n",
    "    prefixes, suffixes, all_combinaisons = generate_basis(model, nb_prefixes, nb_suffixes, max_len, nb_letters)\n",
    "    hankels = create_hankels(model, prefixes, suffixes, all_combinaisons, nb_letters)\n",
    "    \"\"\"Computing the SVD\"\"\"\n",
    "    hankel = hankels[0]\n",
    "    [u, s, v] = svd(hankel)\n",
    "    \n",
    "    u = u[:, :nb_states]\n",
    "    v = v[:nb_states, :]\n",
    "    ds = np.diag(s[:nb_states])\n",
    "    \n",
    "    #Computing WA elements\n",
    "    pis = pinv(v)\n",
    "    del v\n",
    "    pip = pinv(np.dot(u, ds))\n",
    "    del u, ds\n",
    "    init = np.dot(hankel[0, :], pis)\n",
    "    term = np.dot(pip, hankel[:, 0])\n",
    "    transitions = []\n",
    "    for x in range(nb_letters):\n",
    "        hankel = hankels[x+1]\n",
    "        transitions.append(np.dot(pip, np.dot(hankel, pis)))\n",
    "    \n",
    "    WA = sp.Automaton(nbL=nb_letters, nbS=nb_states, initial=init, final=term, transitions=transitions, type=\"classic\")\n",
    "    return WA\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WA = spectral_distillation(model, nb_states=2, nb_prefixes=2, nb_suffixes=2, max_len=10, nb_letters=nb_letters)\n",
    "print(\"Number of states of the extracted WA:\", WA.initial.shape[0])\n",
    "print(\"Output on example:\", WA.val(sequences[42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Save extracted DFA as a MLFlow Model. This is the creation of the model needed for the submission to the competition. \n",
    "\n",
    "The only thing to do is to define a function that takes a sequence as a list of integers and returns the value given to this sequence to the sequence. Your model is **NOT** a parameter of this function. You should **NOT** take care of MLFlow saving here  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(seq):\n",
    "    return WA.val(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and submit \n",
    "This is the creation of the model needed for the submission to the competition: you just have to run this cell. It will create in your current directory an **archive**  that you can then submit on the competition website.\n",
    "\n",
    "**You should NOT modify this part, just run it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit_tools import save_function\n",
    "\n",
    "save_function(predict, alphabet_size=nb_letters, prefix=f'dataset_{TRACK}.{DATASET}_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For fun, show WA graphical representation\n",
    "You may need to install the graphviz library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = WA.get_dot(threshold = 0.01, title = 'dotfile')\n",
    "# To display the dot string, one can use graphviz:\n",
    "from graphviz import Source\n",
    "src = Source(dot)\n",
    "src.render('dotfile' + '.gv', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
