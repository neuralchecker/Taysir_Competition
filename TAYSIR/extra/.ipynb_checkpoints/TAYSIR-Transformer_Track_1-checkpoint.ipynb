{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAYSIR - Transformer for Track 1 (Binary Classification)\n",
    "This notebook gives some clues about our transformers. Unfortunately, we do not have a baseline for transformers trained for classification since there does not exist, as far as we know, an algorithm to extract finite state automata on that task.\n",
    "\n",
    "The only Transformer of Track 1 is Dataset 1.7\n",
    "\n",
    "## Loading the model\n",
    "Our Transformers come from the Hugging Face version of DistilBert. All information can be found here: https://huggingface.co/docs/transformers/model_doc/distilbert\n",
    "\n",
    "First step is thus to make sure you have this installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = 1  #always for this track\n",
    "DATASET = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow, torch, transformers\n",
    "\n",
    "model_name = f\"models/{TRACK}.{DATASET}.taysir.model\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:#RNN\n",
    "    nb_letters = model.input_size -1\n",
    "    cell_type = model.cell_type\n",
    "\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The type of the recurrent cells is\", cell_type.__name__)\n",
    "except: #Transformer \n",
    "    nb_letters = model.distilbert.config.vocab_size\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The model is a transformer (DistilBertForSequenceClassification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful function that takes the transformer and provides its output on a sequence (list of integers, as defined in Taysir): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_transformer(model, word):\n",
    "    \"\"\"\n",
    "    Note: In this function, each id in the word is added to 2 before being input to the model,\n",
    "    since ids 0 and 1 are used as special tokens.\n",
    "        0 : padding id\n",
    "        1 : classification token id\n",
    "    Args:\n",
    "        word: list of integers \n",
    "    \"\"\"\n",
    "    word = [ [1] + [ a+2 for a in word ] ]\n",
    "    word = torch.IntTensor(word)\n",
    "    with torch.no_grad():\n",
    "        out = model(word)\n",
    "        return (out.logits.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = [64, 36, 48, 12, 41, 11, 9, 20, 16, 37, 23, 21, 23, 51, 52, 63, 21, 16, 28, 52, 43, 3, 3, 8, 60, 25, 23, 61, 32, 65]\n",
    "predict_transformer(model, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model extraction\n",
    "\n",
    "This is where you will extract your own model. We do not provide a baseline for transformer of Track 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Save surrogatemodel as a MLFlow Model. This is the creation of the model needed for the submission to the competition. \n",
    "\n",
    "The only thing to do is to define a function that takes a sequence as a list of integers and returns the value given to this sequence to the sequence. Your model is **NOT** a parameter of this function. You should NOT take care of MLFlow saving here  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(seq):\n",
    "    return predict_transformer(model, seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and submit \n",
    "This is the creation of the model needed for the submission to the competition: you just have to run this cell. It will create in your current directory an **archive**  that you can then submit on the competition website.\n",
    "\n",
    "**You should NOT modify this part, just run it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit_tools import save_function\n",
    "\n",
    "save_function(predict, alphabet_size=nb_letters, prefix=f'dataset_{TRACK}.{DATASET}_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
