{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAYSIR competition - Track 1 Starter Kit\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This is a notebook to show the structure of a code to participate to the competition.\n",
    "\n",
    "You can also check the baseline notebook (available in the same archive) for more details about the TAYSIR models and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow in /opt/conda/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (4.27.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.9/site-packages (from mlflow) (3.3.6)\n",
      "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /opt/conda/lib/python3.9/site-packages (from mlflow) (0.17.5)\n",
      "Requirement already satisfied: querystring-parser<2 in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.7.3)\n",
      "Requirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.9/site-packages (from mlflow) (3.5.0)\n",
      "Requirement already satisfied: pytz<2023 in /opt/conda/lib/python3.9/site-packages (from mlflow) (2021.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from mlflow) (0.4.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /opt/conda/lib/python3.9/site-packages (from mlflow) (4.8.2)\n",
      "Requirement already satisfied: Flask<3 in /opt/conda/lib/python3.9/site-packages (from mlflow) (2.2.3)\n",
      "Requirement already satisfied: packaging<24 in /opt/conda/lib/python3.9/site-packages (from mlflow) (23.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.9/site-packages (from mlflow) (8.0.3)\n",
      "Requirement already satisfied: cloudpickle<3 in /opt/conda/lib/python3.9/site-packages (from mlflow) (2.0.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.9/site-packages (from mlflow) (3.0.3)\n",
      "Requirement already satisfied: alembic<2 in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.7.5)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.9/site-packages (from mlflow) (2.26.0)\n",
      "Requirement already satisfied: gunicorn<21 in /opt/conda/lib/python3.9/site-packages (from mlflow) (20.1.0)\n",
      "Requirement already satisfied: docker<7,>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from mlflow) (6.0.1)\n",
      "Requirement already satisfied: shap<1,>=0.40 in /opt/conda/lib/python3.9/site-packages (from mlflow) (0.41.0)\n",
      "Requirement already satisfied: gitpython<4,>=2.1.0 in /opt/conda/lib/python3.9/site-packages (from mlflow) (3.1.24)\n",
      "Requirement already satisfied: pyarrow<12,>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from mlflow) (6.0.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.9/site-packages (from mlflow) (3.19.1)\n",
      "Requirement already satisfied: pandas<3 in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.3.4)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.2.1)\n",
      "Requirement already satisfied: numpy<2 in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.21.4)\n",
      "Requirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.9/site-packages (from mlflow) (0.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.4.27)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.9/site-packages (from mlflow) (6.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.9/site-packages (from torch) (1.9)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch) (4.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.4.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.26.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.9/site-packages (from alembic<2->mlflow) (1.1.6)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.9.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.1.1)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.9/site-packages (from docker<7,>=4.0.0->mlflow) (1.2.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.9/site-packages (from docker<7,>=4.0.0->mlflow) (1.26.7)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/lib/python3.9/site-packages (from Flask<3->mlflow) (2.2.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.9/site-packages (from Flask<3->mlflow) (2.1.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.28.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib<4->mlflow) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib<4->mlflow) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (3.0.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in /opt/conda/lib/python3.9/site-packages (from shap<1,>=0.40->mlflow) (0.0.7)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.9/site-packages (from shap<1,>=0.40->mlflow) (0.56.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (1.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.9/site-packages (from numba->shap<1,>=0.40->mlflow) (0.39.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade mlflow torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "from utils import predict, PytorchInference\n",
    "#print('PyTorch version :', torch.__version__)\n",
    "#print('MLflow version :', mlflow.__version__)\n",
    "import sys\n",
    "import pandas as pd\n",
    "#print(\"Your python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested with:\n",
    "* Torch version: 1.11.0+cu102\n",
    "* MLFlow version: 1.25.1\n",
    "* Python version: 3.8.10 [GCC 9.4.0]\n",
    "\n",
    "Python versions starting at 3.7 are supposed to work (but have not been tested).\n",
    "## Choosing the phase\n",
    "\n",
    "First you must select one of the phases/datasets we provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = 1 #always for his track\n",
    "DATASET = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/25 15:23:13 WARNING mlflow.pytorch: Stored model version '1.13.1+cu117' does not match installed PyTorch version '2.0.0+cu117'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TNetwork(\n",
       "  7, 2, patience=5, bidirectional=True\n",
       "  (mach[0]): RNN(6, 32, batch_first=True, bidirectional=True)\n",
       "  (dense): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "model_name = f\"models/1.{DATASET}.taysir.model\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_name)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alphabet contains 6 symbols.\n",
      "The type of the recurrent cells is RNN\n"
     ]
    }
   ],
   "source": [
    "try:#RNN\n",
    "    nb_letters = model.input_size - 1\n",
    "    cell_type = model.cell_type\n",
    "\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The type of the recurrent cells is\", cell_type.__name__)\n",
    "except:\n",
    "    nb_letters = model.distilbert.config.vocab_size - 2\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The model is a transformer (DistilBertForSequenceClassification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The input data is in the following format :\n",
    "\n",
    "```\n",
    "[Number of sequences] [Alphabet size]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "...\n",
    "[Length of sequence] [List of symbols]\n",
    "```\n",
    "\n",
    "For example the following data :\n",
    "\n",
    "```\n",
    "5 10\n",
    "6 8 6 5 1 6 7 4 9\n",
    "12 8 6 9 4 6 8 2 1 0 6 5 9\n",
    "7 8 9 4 3 0 4 9\n",
    "4 8 0 4 9\n",
    "8 8 1 5 2 6 0 5 3 9\n",
    "```\n",
    "\n",
    "is composed of 5 sequences and has an alphabet size of 10 (so symbols are between 0 and 9) and the first sequence is composed of 6 symbols (8 6 5 1 6 7 4 9), notice that 8 is the start symbol and 9 is the end symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pythautomata.base_types.alphabet import Alphabet\n",
    "\n",
    "file = f\"datasets/1.{DATASET}.taysir.valid.words\"\n",
    "\n",
    "alphabet = None\n",
    "sequences = []\n",
    "\n",
    "#In the competition the empty sequence is defined as [alphabet_size - 2, alphabet size -1]\n",
    "#For example with the alphabet of size 22 the empty sequence is [20, 21]\n",
    "empty_sequence_len = 2\n",
    "\n",
    "with open(file) as f:\n",
    "    a = f.readline() #Skip first line (number of sequences, alphabet size)\n",
    "    headline = a.split(' ')\n",
    "    alphabet_size = int(headline[1].strip())\n",
    "    alphabet = Alphabet.from_strings([str(x) for x in range(alphabet_size - empty_sequence_len)])\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        seq = line.split(' ')\n",
    "        seq = [int(i) for i in seq[1:]] #Remove first value (length of sequence) and cast to int\n",
    "        sequences.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *sequences* is thus **a list of lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 first sequences:\n",
      "10000\n",
      "[4, 2, 1, 3, 1, 3, 0, 1, 2, 2, 3, 2, 2, 3, 2, 1, 3, 1, 2, 0, 2, 5]\n",
      "[4, 0, 0, 0, 0, 0, 3, 3, 1, 2, 1, 3, 3, 1, 1, 0, 0, 2, 2, 0, 3, 5]\n",
      "[4, 0, 1, 2, 3, 0, 3, 2, 0, 3, 2, 1, 0, 3, 2, 2, 2, 1, 0, 1, 2, 5]\n",
      "[4, 0, 3, 0, 1, 3, 3, 3, 0, 1, 3, 2, 0, 2, 0, 2, 2, 3, 3, 3, 1, 5]\n",
      "[4, 3, 1, 0, 1, 0, 3, 2, 2, 0, 1, 1, 0, 0, 0, 1, 3, 0, 1, 2, 2, 5]\n",
      "[4, 2, 2, 3, 2, 0, 0, 2, 2, 2, 2, 1, 3, 0, 0, 3, 2, 2, 3, 3, 1, 5]\n",
      "[4, 3, 1, 2, 0, 1, 3, 3, 0, 0, 2, 1, 1, 3, 0, 2, 0, 1, 0, 3, 2, 5]\n",
      "[4, 1, 0, 1, 0, 0, 0, 0, 0, 3, 0, 2, 2, 3, 0, 2, 1, 1, 2, 3, 2, 5]\n",
      "[4, 3, 2, 3, 0, 2, 0, 3, 0, 3, 2, 1, 2, 1, 1, 2, 2, 2, 3, 1, 0, 5]\n",
      "[4, 3, 1, 1, 1, 2, 0, 3, 3, 1, 2, 3, 1, 1, 0, 1, 0, 3, 0, 1, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "# print('Number of sequences:', len(sequences))\n",
    "print('10 first sequences:')\n",
    "print(len(sequences))\n",
    "for i in range(10):\n",
    "    print(sequences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model extraction\n",
    "\n",
    "This is where you will extract your simple own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Learner, a Comparator, and a Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymodelextractor.teachers.pac_comparison_strategy import PACComparisonStrategy\n",
    "from pymodelextractor.teachers.general_teacher import GeneralTeacher\n",
    "from pymodelextractor.factories.lstar_factory import LStarFactory\n",
    "from pythautomata.utilities.uniform_length_sequence_generator import UniformLengthSequenceGenerator\n",
    "from pymodelextractor.learners.observation_tree_learners.kearns_vazirani_learner import KearnsVaziraniLearner\n",
    "\n",
    "name = \"Track: \" + str(TRACK) + \" - DataSet: \" + str(DATASET)\n",
    "\n",
    "target_model = PytorchInference(alphabet, model, name)\n",
    "\n",
    "sequence_generator = UniformLengthSequenceGenerator(alphabet, max_seq_length=1000, min_seq_length=900)\n",
    "\n",
    "comparator = PACComparisonStrategy(target_model_alphabet = alphabet, epsilon = 0.01, delta = 0.01, \n",
    "                                   sequence_generator = sequence_generator)\n",
    "\n",
    "teacher = GeneralTeacher(target_model, comparator)\n",
    "\n",
    "# Choose algorithm (LStar, KV)\n",
    "algorithm = 'LStar'\n",
    "\n",
    "# LStar learner\n",
    "if(algorithm == 'LStar'): learner = LStarFactory.get_dfa_lstar_learner(max_time=5)\n",
    "\n",
    "#Kearns Vazirani learner\n",
    "if(algorithm == 'KV'):  learner = KearnsVaziraniLearner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Started lstar learning ****\n"
     ]
    }
   ],
   "source": [
    "if(algorithm == 'LStar'): \n",
    "    res = learner.learn(teacher, log_hierachy=1)\n",
    "elif(algorithm == 'KV'): \n",
    "    res = learner.learn(teacher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Observation Table in pickle (only for LStar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('observation_table.pickle', 'rb') as handle:\\n    unserialized_data = pickle.load(handle)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "if(algorithm == 'LStar'):\n",
    "    obs_table = res.info['observation_table']\n",
    "\n",
    "    with open('predicted_models/observation_table.pickle', 'wb') as handle:\n",
    "        pickle.dump(obs_table, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load data example\n",
    "\n",
    "with open('predicted_models/observation_table.pickle', 'rb') as handle:\n",
    "    unserialized_data = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test extracted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import test_model_w_data, test_model\n",
    "import numpy as np\n",
    "\n",
    "result = test_model_w_data(target_model, res.model, sequences)\n",
    "np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.612"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = test_model(target_model, res.model, max_seq_len=1000, min_seq_len=50, sequence_amount=1000)\n",
    "np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = test_model(target_model, res.model, max_seq_len=1000, min_seq_len=900, sequence_amount=1000)\n",
    "np.mean(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final response info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model:\n",
      "- State amount -> 2\n",
      "- Membership queries -> 9\n",
      "- Equivalence queries -> 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Final model:\")\n",
    "print(\"- State amount -> \" + str(len(res.model.states)))\n",
    "print(\"- Membership queries -> \" + str(res.info[\"membership_queries_count\"]))\n",
    "print(\"- Equivalence queries -> \" + str(res.info[\"equivalence_queries_count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence len: 2\n",
      "Table rows: 9\n",
      "Table columns: 1\n"
     ]
    }
   ],
   "source": [
    "max_exp = 0\n",
    "for k in res.info['observation_table'].exp:\n",
    "    if len(k) > max_exp:\n",
    "        max_exp = len(k)\n",
    "        \n",
    "max_blue = 0\n",
    "for k in res.info['observation_table'].blue:\n",
    "    if len(k) > max_blue:\n",
    "        max_blue = len(k)\n",
    "        \n",
    "print(\"Max sequence len: \" + str(max_blue + max_exp))\n",
    "print(\"Table rows: \" + str(len(res.info['observation_table'].blue)+len(res.info['observation_table'].red)))\n",
    "print(\"Table columns: \" + str(len(res.info['observation_table'].exp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Once you are satisfied with your model performance, you must write a predict function that takes a sequence as input (list of integers) and returns 0 or 1 (integer type) for the binary classification track and a probability (float type) for the language modeling track.\n",
    "\n",
    "Your model is **NOT** a parameter of this function. You should NOT take care of MLFlow saving here. \n",
    "\n",
    "*For instance, if you want to submit the original model:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and submit \n",
    "This is the creation of the model needed for the submission to the competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to save predicted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'predicted_models/Track: 1 - DataSet: 1.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34480/2526696508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmlflow_dfa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMlflowDFA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlflow_dfa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/NeuralChecker/TAYSIR/submit_tools_fix.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(func, alphabet_size, prefix)\u001b[0m\n\u001b[1;32m     46\u001b[0m         )\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmlflow_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'predicted_models/Track: 1 - DataSet: 1.zip'"
     ]
    }
   ],
   "source": [
    "from wrapper import MlflowDFA\n",
    "from submit_tools_fix import save_function\n",
    "\n",
    "mlflow_dfa = MlflowDFA(res.model)\n",
    "save_function(mlflow_dfa, len(res.model.alphabet), target_model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.model_exporters.dot_exporters.dfa_dot_exporting_strategy import DfaDotExportingStrategy\n",
    "res.model._exporting_strategies = [DfaDotExportingStrategy()]\n",
    "#res.model.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation Table:\n",
      "=================\n",
      "RED: {0, ϵ}\n",
      "BLUE: {0,3, 3, 1, 0,1, 0,2, 2, 0,0}\n",
      "EXP: [ϵ]\n",
      "OBSERVATIONS: {ϵ: [False], 0: [True], 1: [False], 3: [True], 2: [True], 0,0: [True], 0,1: [False], 0,3: [True], 0,2: [True]}\n",
      "\n",
      "\n",
      "Observation Table:\n",
      "=================\n",
      "RED: {0, ϵ}\n",
      "BLUE: {0,3, 3, 1, 0,1, 0,2, 2, 0,0}\n",
      "EXP: [ϵ]\n",
      "OBSERVATIONS: {ϵ: [False], 0: [True], 1: [False], 3: [True], 2: [True], 0,0: [True], 0,1: [False], 0,3: [True], 0,2: [True]}\n",
      "\n",
      "\n",
      "Observation Table:\n",
      "=================\n",
      "RED: {0, ϵ}\n",
      "BLUE: {0,3, 3, 1, 0,1, 0,2, 2, 0,0}\n",
      "EXP: [ϵ]\n",
      "OBSERVATIONS: {ϵ: [False], 0: [True], 1: [False], 3: [True], 2: [True], 0,0: [True], 0,1: [False], 0,3: [True], 0,2: [True]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "table_0, table_1 = None, None\n",
    "with open('predicted_models/observation_table_0.pickle', 'rb') as handle:\n",
    "    table_0 = pickle.load(handle)\n",
    "    \n",
    "with open('predicted_models/observation_table_1.pickle', 'rb') as handle:\n",
    "    table_1 = pickle.load(handle)\n",
    "    \n",
    "with open('predicted_models/observation_table_2.pickle', 'rb') as handle:\n",
    "    table_2 = pickle.load(handle)\n",
    "    \n",
    "print(table_0)\n",
    "print(table_1)\n",
    "print(table_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/06 03:32:08 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cu117'\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pickle\n",
    "from utils import predict, PytorchInference\n",
    "import numpy as np\n",
    "from wrapper import MlflowDFA\n",
    "from submit_tools_fix import save_function\n",
    "from pythautomata.utilities.uniform_word_sequence_generator import UniformWordSequenceGenerator\n",
    "from pythautomata.model_exporters.dot_exporters.dfa_dot_exporting_strategy import DfaDotExportingStrategy\n",
    "from pymodelextractor.teachers.pac_comparison_strategy import PACComparisonStrategy\n",
    "from pymodelextractor.teachers.general_teacher import GeneralTeacher\n",
    "from pymodelextractor.factories.lstar_factory import LStarFactory\n",
    "from pythautomata.base_types.alphabet import Alphabet\n",
    "from utils import test_model\n",
    "from pymodelextractor.learners.observation_table_learners.translators.partial_dfa_translator import PartialDFATranslator\n",
    "\n",
    "TRACK = 1 #always for his track\n",
    "DATASET = 11\n",
    "\n",
    "max_extraction_time =60# 2 * 60 * 60 # 2 horas\n",
    "max_sequence_len = 80\n",
    "min_sequence_len = 10\n",
    "\n",
    "counter = 0\n",
    "observation_table = None\n",
    "\n",
    "model_name = f\"models/1.{DATASET}.taysir.model\"\n",
    "model = mlflow.pytorch.load_model(model_name)\n",
    "model.eval()\n",
    "\n",
    "file = f\"datasets/1.{DATASET}.taysir.valid.words\"\n",
    "\n",
    "empty_sequence_len = 2\n",
    "with open(file) as f:\n",
    "    a = f.readline() #Skip first line (number of sequences, alphabet size)\n",
    "    headline = a.split(' ')\n",
    "    alphabet_size = int(headline[1].strip())\n",
    "    alphabet = Alphabet.from_strings([str(x) for x in range(alphabet_size - empty_sequence_len)])\n",
    "\n",
    "name = \"Track: \" + str(TRACK) + \" - DataSet: \" + str(DATASET) + \"-  partial n° \" + str(counter)\n",
    "target_model = PytorchInference(alphabet, model, name)\n",
    "\n",
    "sequence_generator = UniformWordSequenceGenerator(alphabet, max_seq_length=max_sequence_len,\n",
    "                                                        min_seq_length=min_sequence_len)\n",
    "\n",
    "comparator = PACComparisonStrategy(target_model_alphabet = alphabet, epsilon = 0.01, delta = 0.01,\n",
    "                                   sequence_generator = sequence_generator)\n",
    "\n",
    "teacher = GeneralTeacher(target_model, comparator)\n",
    "\n",
    "learner = LStarFactory.get_partial_dfa_lstar_learner(max_time=max_extraction_time)\n",
    "\n",
    "name = \"Track: \" + str(TRACK) + \" - DataSet: \" + str(DATASET) + \"-  partial n° \" + str(counter)\n",
    "res = learner.learn(teacher, observation_table)\n",
    "\n",
    "#print(observation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission created at predicted_models/Track: 1 - DataSet: 11-  partial n° 0.zip.\n"
     ]
    }
   ],
   "source": [
    "from wrapper import MlflowDFA\n",
    "from submit_tools_fix import save_function\n",
    "\n",
    "mlflow_dfa = MlflowDFA(res.model)\n",
    "save_function(mlflow_dfa, len(res.model.alphabet), target_model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'equivalence_queries_count': 1, 'membership_queries_count': 26, 'observation_table': <pymodelextractor.learners.observation_table_learners.general_observation_table.GeneralObservationTable object at 0x7f2440c0e4f0>, 'duration': 60}\n"
     ]
    }
   ],
   "source": [
    "print(res.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(res.model.states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.model.name = \"Dataset11-1Acc\"\n",
    "res.model.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NXdKde0kt3FR",
    "eKzzh3hot9vZ",
    "BMQF46fnw1Zk"
   ],
   "name": "PFA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "122e9f251b80a4a76e7262659287020d96f7188da42b39e3d812967db6c8742d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
