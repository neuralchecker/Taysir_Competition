{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAYSIR competition - Track 2 Example Extraction\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This is a notebook to show the structure of a code to participate to the competition.\n",
    "\n",
    "You can also check the baseline notebook (available in the same archive) for more details about the TAYSIR models and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version : 2.0.0+cpu\n",
      "MLflow version : 2.2.2\n",
      "Your python version: 3.9.1 (default, Dec 11 2020, 09:29:25) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "print('PyTorch version :', torch.__version__)\n",
    "print('MLflow version :', mlflow.__version__)\n",
    "import sys\n",
    "print(\"Your python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested with:\n",
    "* Torch version: 1.11.0+cu102\n",
    "* MLFlow version: 1.25.1\n",
    "* Python version: 3.8.10 [GCC 9.4.0]\n",
    "\n",
    "Python versions starting at 3.7 are supposed to work (but have not been tested).\n",
    "## Choosing the phase\n",
    "\n",
    "First you must select one of the phases/datasets we provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = 2 #always for this track\n",
    "DATASET = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/23 00:13:51 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TNetwork(\n",
       "  23, 22, n_layers=2, neurons_per_layer=64, batch_size=64, patience=5, split_dense=True, task=lm\n",
       "  (mach[0]): RNN(22, 64, batch_first=True)\n",
       "  (mach[1]): RNN(64, 64, batch_first=True)\n",
       "  (dense): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): Linear(in_features=32, out_features=22, bias=True)\n",
       "    (2): Sigmoid()\n",
       "    (3): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = f\"models/{TRACK}.{DATASET}.taysir.model\"\n",
    "model = mlflow.pytorch.load_model(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is RNN\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(model, 'distilbert'):#RNN\n",
    "    nb_letters = model.input_size -1\n",
    "    cell_type = model.cell_type\n",
    "\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The type of the recurrent cells is\", cell_type.__name__)\n",
    "else:\n",
    "    nb_letters = model.distilbert.config.vocab_size\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The model is a transformer (DistilBertForSequenceClassification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The input data is in the following format :\n",
    "\n",
    "```\n",
    "[Number of sequences] [Alphabet size]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "...\n",
    "[Length of sequence] [List of symbols]\n",
    "```\n",
    "\n",
    "For example the following data :\n",
    "\n",
    "```\n",
    "5 10\n",
    "6 8 6 5 1 6 7 4 9\n",
    "12 8 6 9 4 6 8 2 1 0 6 5 9\n",
    "7 8 9 4 3 0 4 9\n",
    "4 8 0 4 9\n",
    "8 8 1 5 2 6 0 5 3 9\n",
    "```\n",
    "\n",
    "is composed of 5 sequences and has an alphabet size of 10 (so symbols are between 0 and 9) and the first sequence is composed of 6 symbols (8 6 5 1 6 7 4 9), notice that 8 is the start symbol and 9 is the end symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.base_types.alphabet import Alphabet\n",
    "\n",
    "file = f\"datasets/{TRACK}.{DATASET}.taysir.valid.words\"\n",
    "\n",
    "alphabet = None\n",
    "sequences = []\n",
    "\n",
    "#In the competition the empty sequence is defined as [alphabet_size - 2, alphabet size -1]\n",
    "#For example with the alphabet of size 22 the empty sequence is [20, 21]\n",
    "empty_sequence_len = 2\n",
    "\n",
    "with open(file) as f:\n",
    "    a = f.readline() #Skip first line (number of sequences, alphabet size)\n",
    "    headline = a.split(' ')\n",
    "    alphabet_size = int(headline[1].strip())\n",
    "    alphabet = Alphabet.from_strings([str(x) for x in range(alphabet_size - 2)])\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        seq = line.split(' ')\n",
    "        seq = [int(i) for i in seq[1:]] #Remove first value (length of sequence) and cast to int\n",
    "        sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({9, 10, 0, 15, 7, 8, 14, 3, 1, 4, 19, 13, 11, 16, 17, 6, 5, 12, 2, 18})\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(alphabet)\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *sequences* is thus **a list of lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 9090\n",
      "10 first sequences:\n",
      "[20, 13, 14, 6, 0, 15, 4, 3, 5, 12, 13, 13, 14, 4, 12, 17, 21]\n",
      "[20, 3, 13, 3, 16, 6, 4, 13, 1, 21]\n",
      "[20, 13, 6, 15, 21]\n",
      "[20, 13, 10, 3, 21]\n",
      "[20, 13, 10, 3, 16, 6, 4, 13, 13, 12, 17, 4, 13, 14, 10, 0, 10, 13, 14, 4, 15, 12, 17, 21]\n",
      "[20, 3, 5, 0, 1, 4, 13, 6, 14, 4, 14, 4, 14, 13, 10, 12, 1, 5, 10, 3, 14, 5, 12, 14, 1, 12, 11, 12, 17, 18, 8, 21]\n",
      "[20, 3, 13, 3, 19, 1, 4, 3, 5, 10, 3, 19, 8, 21]\n",
      "[20, 13, 0, 1, 3, 1, 13, 3, 16, 6, 4, 13, 1, 12, 8, 0, 5, 10, 14, 12, 10, 3, 14, 1, 21]\n",
      "[20, 13, 14, 14, 6, 3, 21]\n",
      "[20, 13, 12, 13, 3, 16, 3, 16, 21]\n"
     ]
    }
   ],
   "source": [
    "print('Number of sequences:', len(sequences))\n",
    "print('10 first sequences:')\n",
    "for i in range(10):\n",
    "    print(sequences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np        \n",
    "# Defining the dict\n",
    "d = defaultdict(lambda: 0)\n",
    "lengths_orig = []\n",
    "for seq in sequences:\n",
    "    lengths_orig.append(len(seq))\n",
    "    d[len(seq)]+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.954565456545655"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lengths_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 9090\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Number of sequences:', len(sequences))\n",
    "import matplotlib.pyplot as plt\n",
    "x = plt.hist(lengths_orig, bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import random\n",
    "\n",
    "def generate_seq(model, cache):\n",
    "    dummy_seq = [len(alphabet)-1, len(alphabet)]\n",
    "    next_probas = utils.full_next_symbols_probas(dummy_seq, model)[1]\n",
    "    seq  = [len(alphabet)-1]    \n",
    "    next_symbol = random.choices(list(range(len(alphabet)+3)), weights = next_probas, k = 1)\n",
    "    seq.extend(next_symbol)    \n",
    "    while next_symbol[0]!=len(alphabet)+2:\n",
    "        if tuple(seq) in cache:\n",
    "            next_probas = cache[tuple(seq)]\n",
    "        else:\n",
    "            next_probas = utils.next_symbols_probas(seq, model)\n",
    "            cache[tuple(seq)] = next_probas\n",
    "        next_symbol = random.choices(list(range(len(alphabet)+3)), weights = next_probas, k = 1)\n",
    "        seq.extend(next_symbol)        \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 125/1000 [04:03<28:25,  1.95s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\generate_datasets_with_LM.ipynb Cell 19\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y100sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lengths \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y100sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     generated_seq \u001b[39m=\u001b[39m generate_seq(model, cache)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y100sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     new_valid_seqs\u001b[39m.\u001b[39mappend(generated_seq)    \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y100sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     lengths\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m(generated_seq))\n",
      "\u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\generate_datasets_with_LM.ipynb Cell 19\u001b[0m in \u001b[0;36mgenerate_seq\u001b[1;34m(model, cache)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y100sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     next_probas \u001b[39m=\u001b[39m cache[\u001b[39mtuple\u001b[39m(seq)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y100sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y100sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     next_probas \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mnext_symbols_probas(seq, model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y100sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     cache[\u001b[39mtuple\u001b[39m(seq)] \u001b[39m=\u001b[39m next_probas\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y100sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m next_symbol \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoices(\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(alphabet)\u001b[39m+\u001b[39m\u001b[39m3\u001b[39m)), weights \u001b[39m=\u001b[39m next_probas, k \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:93\u001b[0m, in \u001b[0;36mnext_symbols_probas\u001b[1;34m(sequence, model)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext_symbols_probas\u001b[39m(sequence, model):     \n\u001b[1;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m full_next_symbols_probas(sequence, model)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:29\u001b[0m, in \u001b[0;36mfull_next_symbols_probas\u001b[1;34m(sequence, model)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_next_symbols_probas\u001b[39m(sequence, model):   \n\u001b[1;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m full_next_symbols_probas_batch([sequence],model)[\u001b[39m0\u001b[39m]   \n\u001b[0;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m'\u001b[39m\u001b[39mdistilbert\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     31\u001b[0m         value, hiddens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward_lm(model\u001b[39m.\u001b[39mone_hot_encode(sequence))\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:62\u001b[0m, in \u001b[0;36mfull_next_symbols_probas_batch\u001b[1;34m(sequences, model)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m'\u001b[39m\u001b[39mdistilbert\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     61\u001b[0m     sequences \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: model\u001b[39m.\u001b[39mone_hot_encode(x), sequences)))             \n\u001b[1;32m---> 62\u001b[0m     value, hiddens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward_lm(sequences)        \n\u001b[0;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m value\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     65\u001b[0m \u001b[39melse\u001b[39;00m: \u001b[39m#Transformer\u001b[39;00m\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.1.taysir.model\\code\\tnetwork.py:178\u001b[0m, in \u001b[0;36mTNetwork.forward_lm\u001b[1;34m(self, x, hidden, full_ret)\u001b[0m\n\u001b[0;32m    176\u001b[0m word_h \u001b[39m=\u001b[39m []\n\u001b[0;32m    177\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)]:\n\u001b[1;32m--> 178\u001b[0m     ohx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_bin(word[:i], h_0, full_ret)\n\u001b[0;32m    179\u001b[0m     o, h \u001b[39m=\u001b[39m ohx[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    180\u001b[0m     \u001b[39mif\u001b[39;00m is_batched:\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.1.taysir.model\\code\\tnetwork.py:144\u001b[0m, in \u001b[0;36mTNetwork.forward_bin\u001b[1;34m(self, x, hidden, full_ret)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_bin\u001b[39m(\u001b[39mself\u001b[39m, x, hidden\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, full_ret\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    143\u001b[0m     words \u001b[39m=\u001b[39m unpad(unbatch(x))\n\u001b[1;32m--> 144\u001b[0m     states \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pass_recurrent(word, hidden, b\u001b[39m=\u001b[39mb)\n\u001b[0;32m    145\u001b[0m               \u001b[39mfor\u001b[39;00m b,word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words)]\n\u001b[0;32m    146\u001b[0m     outs, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_states(states)\n\u001b[0;32m    147\u001b[0m     fed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_dense(torch\u001b[39m.\u001b[39mstack(outs))\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.1.taysir.model\\code\\tnetwork.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_bin\u001b[39m(\u001b[39mself\u001b[39m, x, hidden\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, full_ret\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    143\u001b[0m     words \u001b[39m=\u001b[39m unpad(unbatch(x))\n\u001b[1;32m--> 144\u001b[0m     states \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pass_recurrent(word, hidden, b\u001b[39m=\u001b[39;49mb)\n\u001b[0;32m    145\u001b[0m               \u001b[39mfor\u001b[39;00m b,word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words)]\n\u001b[0;32m    146\u001b[0m     outs, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_states(states)\n\u001b[0;32m    147\u001b[0m     fed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_dense(torch\u001b[39m.\u001b[39mstack(outs))\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.1.taysir.model\\code\\tnetwork.py:324\u001b[0m, in \u001b[0;36mTNetwork._pass_recurrent\u001b[1;34m(self, word, hidden, b)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhides_pairs \u001b[39mand\u001b[39;00m h\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    323\u001b[0m         h \u001b[39m=\u001b[39m h[:,b]\n\u001b[1;32m--> 324\u001b[0m out \u001b[39m=\u001b[39m machine(out[\u001b[39m0\u001b[39m], h, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    326\u001b[0m     out \u001b[39m=\u001b[39m (\u001b[39m*\u001b[39mout, out[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:509\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_TANH\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 509\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mrnn_tanh(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    510\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional,\n\u001b[0;32m    511\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    512\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    513\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mrnn_relu(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    514\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional,\n\u001b[0;32m    515\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "new_valid_seqs = []\n",
    "lengths = []\n",
    "for i in tqdm(range(1000)):\n",
    "    generated_seq = generate_seq(model, cache)\n",
    "    new_valid_seqs.append(generated_seq)    \n",
    "    lengths.append(len(generated_seq))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\generate_datasets_with_LM.ipynb Cell 20\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mhist(lengths, bins \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#Y103sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m np\u001b[39m.\u001b[39mmean(lengths)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lengths' is not defined"
     ]
    }
   ],
   "source": [
    "x = plt.hist(lengths, bins = 100)\n",
    "np.mean(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model extraction\n",
    "\n",
    "This is where you will extract your simple own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.model_comparators.wfa_partition_comparison_strategy import WFAPartitionComparator\n",
    "from pythautomata.utilities.probability_partitioner import QuantizationProbabilityPartitioner\n",
    "#from pythautomata.model_exporters.wfa_image_exporter_with_partition_mapper import WFAImageExporterWithPartitionMapper\n",
    "from pythautomata.base_types.symbol import SymbolStr\n",
    "from pythautomata.utilities.uniform_length_sequence_generator import UniformLengthSequenceGenerator\n",
    "\n",
    "from pymodelextractor.learners.observation_tree_learners.bounded_pdfa_quantization_n_ary_tree_learner import BoundedPDFAQuantizationNAryTreeLearner\n",
    "from pymodelextractor.teachers.pac_batch_probabilistic_teacher import PACBatchProbabilisticTeacher\n",
    "from pymodelextractor.teachers.pac_probabilistic_teacher import PACProbabilisticTeacher\n",
    "from pymodelextractor.utils.pickle_data_loader import PickleDataLoader\n",
    "\n",
    "from utils import predict\n",
    "from pytorch_language_model import PytorchLanguageModel\n",
    "\n",
    "name = \"track_\" + str(TRACK) + \"_dataset_\" + str(DATASET)\n",
    "\n",
    "target_model = PytorchLanguageModel(alphabet, model, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.base_types.alphabet import Alphabet\n",
    "from pythautomata.base_types.symbol import SymbolStr\n",
    "from pythautomata.base_types.sequence import Sequence\n",
    "\n",
    "def get_alphabet_and_validation_sequences(ds):\n",
    "    file = f\"datasets/2.{ds}.taysir.valid.words\"\n",
    "    alphabet = None\n",
    "    sequences = []\n",
    "\n",
    "    empty_sequence_len = 2\n",
    "    with open(file) as f:\n",
    "        a = f.readline()\n",
    "        headline = a.split(' ')\n",
    "        alphabet_size = int(headline[1].strip())\n",
    "        alphabet = Alphabet.from_strings([str(x) for x in range(alphabet_size - empty_sequence_len)])\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            seq = line.split(' ')[1:]\n",
    "            seq = [SymbolStr(i) for i in seq[1:]]            \n",
    "            sequences.append(Sequence(seq))\n",
    "    return alphabet, sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pythaut_sequences = get_alphabet_and_validation_sequences(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = [target_model.terminal_symbol]\n",
    "for symbol in target_model.alphabet.symbols:\n",
    "    suffixes.append(Sequence((symbol,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_0 = Sequence([SymbolStr('0')])\n",
    "seq_010 = Sequence([SymbolStr('0'), SymbolStr('1'), SymbolStr('0')])\n",
    "seq_eps = Sequence([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "0.9213731437921524\n",
      "(0, 0.0)\n",
      "(1, 0.03931346535682678)\n",
      "(2, 0.039313968271017075)\n",
      "(3, 0.039313461631536484)\n",
      "(4, 0.1068650633096695)\n",
      "(5, 0.039313461631536484)\n",
      "(6, 0.039313461631536484)\n",
      "(7, 0.039313461631536484)\n",
      "(8, 0.039313461631536484)\n",
      "(9, 0.039313461631536484)\n",
      "(10, 0.039313461631536484)\n",
      "(11, 0.039313528686761856)\n",
      "(12, 0.039313461631536484)\n",
      "(13, 0.03931359201669693)\n",
      "(14, 0.1068650633096695)\n",
      "(15, 0.039313461631536484)\n",
      "(16, 0.039313461631536484)\n",
      "(17, 0.039313461631536484)\n",
      "(18, 0.039313461631536484)\n",
      "(19, 0.039313461631536484)\n",
      "(21, 0.039313461631536484)\n"
     ]
    }
   ],
   "source": [
    "symbols = list(target_model.alphabet.symbols)\n",
    "symbols.sort()\n",
    "symbols = [target_model.terminal_symbol] + symbols\n",
    "res = target_model.get_last_token_weights(seq_eps, symbols)\n",
    "print(len(res))\n",
    "print(np.sum(res))\n",
    "res = list(zip(symbols, res))\n",
    "res.sort(key=lambda x: int(x[0].value))\n",
    "for i in res:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.039313461631536484"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.sequence_probability(seq_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.039313461631536484"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "utils.sequence_probability([20,21], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "0.9999999\n",
      "(0, 0.0)\n",
      "(1, 0.039313465)\n",
      "(2, 0.03931397)\n",
      "(3, 0.03931346)\n",
      "(4, 0.10686506)\n",
      "(5, 0.03931346)\n",
      "(6, 0.03931346)\n",
      "(7, 0.03931346)\n",
      "(8, 0.03931346)\n",
      "(9, 0.03931346)\n",
      "(10, 0.03931346)\n",
      "(11, 0.03931353)\n",
      "(12, 0.03931346)\n",
      "(13, 0.039313592)\n",
      "(14, 0.10686506)\n",
      "(15, 0.03931346)\n",
      "(16, 0.03931346)\n",
      "(17, 0.03931346)\n",
      "(18, 0.03931346)\n",
      "(19, 0.03931346)\n",
      "(20, 0.03931346)\n",
      "(21, 0.03931346)\n",
      "(22, 0.03931346)\n"
     ]
    }
   ],
   "source": [
    "a = utils.next_symbols_probas([20,20], model)\n",
    "print(len(a))\n",
    "print(np.sum(a))\n",
    "res = list(zip(range(len(a)), a))\n",
    "res.sort(key=lambda x: x[0])\n",
    "for i in res:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 0, 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 9, 15, 17, 13, 19, 16, 11, 0, 5, 8, 10, 6, 7, 1, 12, 3, 2, 4, 14, 18]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.039313592,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931353,\n",
       "  0.0,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.039313465,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931397,\n",
       "  0.10686506,\n",
       "  0.10686506,\n",
       "  0.03931346]]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.get_last_token_weights_batch([seq_eps], suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from last_token_weights_pickle_dataset_generator import LastTokenWeightsPickleDataSetGenerator\n",
    "#LastTokenWeightsPickleDataSetGenerator().genearte_dataset(target_model, 1000, \"./test\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pythautomata.automata.wheighted_automaton_definition.probabilistic_deterministic_finite_automaton import \\\n",
    "#    ProbabilisticDeterministicFiniteAutomaton as PDFA\n",
    "#from pythautomata.automata.wheighted_automaton_definition.weighted_state import WeightedState\n",
    "#from pythautomata.model_comparators.wfa_tolerance_comparison_strategy import WFAToleranceComparator\n",
    "\n",
    "#weight = 1.0/(len(alphabet)+1)\n",
    "#weight = 0\n",
    "#q0 = WeightedState(\"q0\", 1, weight)\n",
    "#for symbol in alphabet.symbols:\n",
    "#    q0.add_transition(symbol, q0, weight)\n",
    "#states = {q0}\n",
    "#comparator = WFAToleranceComparator()\n",
    "#testPDFA = PDFA(alphabet, states, target_model.terminal_symbol, comparator, \"Test\", check_is_probabilistic = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testPDFA.sequence_probability(seq_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\generate_datasets_with_LM.ipynb Cell 39\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m teacher1  \u001b[39m=\u001b[39m SampleBatchProbabilisticTeacher(model \u001b[39m=\u001b[39m target_model, comparator \u001b[39m=\u001b[39m comparator, sequence_generator\u001b[39m=\u001b[39msequence_generator, max_seq_length\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, full_prefix_set\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,  cache_from_dataloader\u001b[39m=\u001b[39mdataloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m learner \u001b[39m=\u001b[39m BoundedPDFAQuantizationNAryTreeLearner(partitioner, max_states, max_query_length, max_secs, generate_partial_hipothesis \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, pre_cache_queries_for_building_hipothesis \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,  check_probabilistic_hipothesis \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/generate_datasets_with_LM.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m learning_result \u001b[39m=\u001b[39m learner\u001b[39m.\u001b[39;49mlearn(teacher1)\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\bounded_pdfa_quantization_n_ary_tree_learner.py:50\u001b[0m, in \u001b[0;36mBoundedPDFAQuantizationNAryTreeLearner.learn\u001b[1;34m(self, teacher, verbose)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_learning_with_time_bound(teacher, verbose)\n\u001b[0;32m     49\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m         \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(teacher, verbose)\n\u001b[0;32m     51\u001b[0m \u001b[39mexcept\u001b[39;00m NumberOfStatesExceededException:\n\u001b[0;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumberOfStatesExceeded\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\pdfa_quantization_n_ary_tree_learner.py:91\u001b[0m, in \u001b[0;36mPDFAQuantizationNAryTreeLearner.learn\u001b[1;34m(self, teacher, verbose)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSize before update:\u001b[39m\u001b[39m'\u001b[39m, last_size)                \n\u001b[0;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_tree(counterexample, model)\n\u001b[1;32m---> 91\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtentative_hypothesis()\n\u001b[0;32m     92\u001b[0m models\u001b[39m.\u001b[39mappend(model)\n\u001b[0;32m     93\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mweighted_states)\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\pdfa_quantization_n_ary_tree_learner.py:145\u001b[0m, in \u001b[0;36mPDFAQuantizationNAryTreeLearner.tentative_hypothesis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mfor\u001b[39;00m access_string, state \u001b[39min\u001b[39;00m states\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    144\u001b[0m     \u001b[39mfor\u001b[39;00m symbol \u001b[39min\u001b[39;00m symbols:\n\u001b[1;32m--> 145\u001b[0m         access_string_of_transition, updated_tree \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tree\u001b[39m.\u001b[39;49msift(access_string \u001b[39m+\u001b[39;49m symbol)\n\u001b[0;32m    146\u001b[0m         \u001b[39mif\u001b[39;00m updated_tree:\n\u001b[0;32m    147\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\pdfa_quantization_n_ary_tree_learner.py:246\u001b[0m, in \u001b[0;36mClassificationTree.sift\u001b[1;34m(self, sequence, update)\u001b[0m\n\u001b[0;32m    244\u001b[0m d \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mstring\n\u001b[0;32m    245\u001b[0m sd \u001b[39m=\u001b[39m sequence \u001b[39m+\u001b[39m d\n\u001b[1;32m--> 246\u001b[0m sd_probabilities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_token_probabilities(sd, update)\u001b[39m.\u001b[39mvalues()\n\u001b[0;32m    247\u001b[0m child_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_look_for_branch(node\u001b[39m.\u001b[39mchilds, \u001b[39mlist\u001b[39m(sd_probabilities))\n\u001b[0;32m    248\u001b[0m \u001b[39mif\u001b[39;00m child_key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\pdfa_quantization_n_ary_tree_learner.py:278\u001b[0m, in \u001b[0;36mClassificationTree._next_token_probabilities\u001b[1;34m(self, sequence, check_max_query_length)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_token_probabilities_cache[sequence]\n\u001b[0;32m    277\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 278\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_teacher\u001b[39m.\u001b[39;49mnext_token_probabilities(sequence)\n\u001b[0;32m    279\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_token_probabilities_cache[sequence] \u001b[39m=\u001b[39m value\n\u001b[0;32m    280\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\teachers\\probabilistic_teacher.py:57\u001b[0m, in \u001b[0;36mProbabilisticTeacher.next_token_probabilities\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     55\u001b[0m symbols\u001b[39m.\u001b[39msort()\n\u001b[0;32m     56\u001b[0m symbols \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mterminal_symbol] \u001b[39m+\u001b[39m symbols\n\u001b[1;32m---> 57\u001b[0m probabilities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_token_weights(sequence, symbols)\n\u001b[0;32m     58\u001b[0m probabilities \u001b[39m=\u001b[39m OrderedDict(\u001b[39mzip\u001b[39m(symbols, probabilities))\n\u001b[0;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m probabilities\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\teachers\\probabilistic_teacher.py:45\u001b[0m, in \u001b[0;36mProbabilisticTeacher.last_token_weights\u001b[1;34m(self, sequence, required_suffixes)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlast_token_weights\u001b[39m(\u001b[39mself\u001b[39m, sequence: Sequence, required_suffixes: \u001b[39mlist\u001b[39m[Sequence]):\n\u001b[0;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_token_weight_queries_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(required_suffixes)            \n\u001b[1;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_target_model\u001b[39m.\u001b[39;49mget_last_token_weights(sequence, required_suffixes)\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pytorch_language_model.py:107\u001b[0m, in \u001b[0;36mPytorchLanguageModel.get_last_token_weights\u001b[1;34m(self, sequence, required_suffixes)\u001b[0m\n\u001b[0;32m    105\u001b[0m         new_prefix \u001b[39m=\u001b[39m Sequence(new_sequence[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m    106\u001b[0m         new_suffix \u001b[39m=\u001b[39m new_sequence[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 107\u001b[0m         next_symbol_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_symbol_probas(new_prefix)\n\u001b[0;32m    108\u001b[0m         weights\u001b[39m.\u001b[39mappend(next_symbol_weights[new_suffix])\n\u001b[0;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m weights\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pytorch_language_model.py:67\u001b[0m, in \u001b[0;36mPytorchLanguageModel.next_symbol_probas\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext_symbol_probas\u001b[39m(\u001b[39mself\u001b[39m, sequence: Sequence):\n\u001b[0;32m     63\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m    Function that returns a dictionary with the probability of next symbols (not including padding_symbol)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m    Quickly implemented, depends on raw_next_symbol_probas(sequence) \u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m                \n\u001b[1;32m---> 67\u001b[0m     next_probas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_next_symbol_probas(sequence)\n\u001b[0;32m     69\u001b[0m     symbols \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphabet\u001b[39m.\u001b[39msymbols) \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_terminal_symbol]\n\u001b[0;32m     70\u001b[0m     intermediate_dict \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pytorch_language_model.py:56\u001b[0m, in \u001b[0;36mPytorchLanguageModel.raw_next_symbol_probas\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraw_next_symbol_probas\u001b[39m(\u001b[39mself\u001b[39m, sequence: Sequence):\n\u001b[1;32m---> 56\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_query(sequence)        \n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pytorch_language_model.py:39\u001b[0m, in \u001b[0;36mPytorchLanguageModel.process_query\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mfull_next_symbols_probas(adapted_sequence, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mnext_symbols_probas(adapted_sequence, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model)\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:93\u001b[0m, in \u001b[0;36mnext_symbols_probas\u001b[1;34m(sequence, model)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext_symbols_probas\u001b[39m(sequence, model):     \n\u001b[1;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m full_next_symbols_probas(sequence, model)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:29\u001b[0m, in \u001b[0;36mfull_next_symbols_probas\u001b[1;34m(sequence, model)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_next_symbols_probas\u001b[39m(sequence, model):   \n\u001b[1;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m full_next_symbols_probas_batch([sequence],model)[\u001b[39m0\u001b[39m]   \n\u001b[0;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m'\u001b[39m\u001b[39mdistilbert\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     31\u001b[0m         value, hiddens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward_lm(model\u001b[39m.\u001b[39mone_hot_encode(sequence))\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:62\u001b[0m, in \u001b[0;36mfull_next_symbols_probas_batch\u001b[1;34m(sequences, model)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m'\u001b[39m\u001b[39mdistilbert\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     61\u001b[0m     sequences \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: model\u001b[39m.\u001b[39mone_hot_encode(x), sequences)))             \n\u001b[1;32m---> 62\u001b[0m     value, hiddens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward_lm(sequences)        \n\u001b[0;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m value\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     65\u001b[0m \u001b[39melse\u001b[39;00m: \u001b[39m#Transformer\u001b[39;00m\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.1.taysir.model\\code\\tnetwork.py:178\u001b[0m, in \u001b[0;36mTNetwork.forward_lm\u001b[1;34m(self, x, hidden, full_ret)\u001b[0m\n\u001b[0;32m    176\u001b[0m word_h \u001b[39m=\u001b[39m []\n\u001b[0;32m    177\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)]:\n\u001b[1;32m--> 178\u001b[0m     ohx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_bin(word[:i], h_0, full_ret)\n\u001b[0;32m    179\u001b[0m     o, h \u001b[39m=\u001b[39m ohx[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    180\u001b[0m     \u001b[39mif\u001b[39;00m is_batched:\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.1.taysir.model\\code\\tnetwork.py:146\u001b[0m, in \u001b[0;36mTNetwork.forward_bin\u001b[1;34m(self, x, hidden, full_ret)\u001b[0m\n\u001b[0;32m    143\u001b[0m words \u001b[39m=\u001b[39m unpad(unbatch(x))\n\u001b[0;32m    144\u001b[0m states \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pass_recurrent(word, hidden, b\u001b[39m=\u001b[39mb)\n\u001b[0;32m    145\u001b[0m           \u001b[39mfor\u001b[39;00m b,word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words)]\n\u001b[1;32m--> 146\u001b[0m outs, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_states(states)\n\u001b[0;32m    147\u001b[0m fed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_dense(torch\u001b[39m.\u001b[39mstack(outs))\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.1.taysir.model\\code\\tnetwork.py:278\u001b[0m, in \u001b[0;36mTNetwork._interpret_states\u001b[1;34m(self, states)\u001b[0m\n\u001b[0;32m    276\u001b[0m     hidden \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mhidden))\n\u001b[0;32m    277\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 278\u001b[0m     hidden \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mstack(h,\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m states]\n\u001b[0;32m    279\u001b[0m outs \u001b[39m=\u001b[39m [v\u001b[39m.\u001b[39mflatten() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m outs]\n\u001b[0;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m outs, hidden\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.1.taysir.model\\code\\tnetwork.py:278\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    276\u001b[0m     hidden \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mhidden))\n\u001b[0;32m    277\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 278\u001b[0m     hidden \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49mstack(h,\u001b[39m1\u001b[39;49m) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m states]\n\u001b[0;32m    279\u001b[0m outs \u001b[39m=\u001b[39m [v\u001b[39m.\u001b[39mflatten() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m outs]\n\u001b[0;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m outs, hidden\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from pymodelextractor.teachers.sample_batch_probabilistic_teacher import SampleBatchProbabilisticTeacher\n",
    "epsilon = 0.1\n",
    "delta = 0.1\n",
    "max_states = 1000000\n",
    "max_query_length= 1000000\n",
    "max_secs = None\n",
    "sequence_generator = UniformLengthSequenceGenerator(alphabet, max_seq_length=2, min_seq_length=2)\n",
    "dataloader = PickleDataLoader(\"./data_caches/\"+target_model.name) \n",
    "\n",
    "partitioner = QuantizationProbabilityPartitioner(1000)\n",
    "comparator = WFAPartitionComparator(partitioner)\n",
    "teacher1  = SampleBatchProbabilisticTeacher(model = target_model, comparator = comparator, sequence_generator=sequence_generator, max_seq_length=2, full_prefix_set=True,  cache_from_dataloader=dataloader)\n",
    "learner = BoundedPDFAQuantizationNAryTreeLearner(partitioner, max_states, max_query_length, max_secs, generate_partial_hipothesis = True, pre_cache_queries_for_building_hipothesis = False,  check_probabilistic_hipothesis = False)\n",
    "learning_result = learner.learn(teacher1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(learning_result.model.weighted_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_model = learning_result.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_model.sequence_probability(seq_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012922859750688076"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.sequence_probability(seq_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = list()\n",
    "suffixes.append(Sequence() + learned_model.terminal_symbol)\n",
    "\n",
    "for symbol in learned_model.alphabet.symbols:\n",
    "    suffixes.append(Sequence((symbol,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(np.array(learned_model.last_token_probabilities(seq_010, suffixes)) - np.array(target_model.last_token_probabilities(seq_010, suffixes)))\n",
    "0.06757163628935814 < 1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_model.sequence_probability(seq_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ϵ\n",
      "Terminal symbol prob: 0.039313461631536484\n",
      "0 0.0\n",
      "1 0.03931346535682678\n",
      "10 0.039313461631536484\n",
      "11 0.039313528686761856\n",
      "12 0.039313461631536484\n",
      "13 0.03931359201669693\n",
      "14 0.1068650633096695\n",
      "15 0.039313461631536484\n",
      "16 0.039313461631536484\n",
      "17 0.039313461631536484\n",
      "18 0.039313461631536484\n",
      "19 0.039313461631536484\n",
      "2 0.039313968271017075\n",
      "3 0.039313461631536484\n",
      "4 0.1068650633096695\n",
      "5 0.039313461631536484\n",
      "6 0.039313461631536484\n",
      "7 0.039313461631536484\n",
      "8 0.039313461631536484\n",
      "9 0.039313461631536484\n"
     ]
    }
   ],
   "source": [
    "for state in learned_model.weighted_states:\n",
    "    if len(state.name) <1:\n",
    "        print(state.name)\n",
    "        print(\"Terminal symbol prob:\", state.final_weight)\n",
    "        for symbol, weighted_transitions in state.transitions_list.items():\n",
    "            print(symbol, weighted_transitions[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teacher2  = PACBatchProbabilisticTeacher(target_model, epsilon = epsilon, delta = delta, max_seq_length = None, comparator = comparator, sequence_generator=sequence_generator, compute_epsilon_star=False, cache_from_dataloader=dataloader)\n",
    "#learner2 = BoundedPDFAQuantizationNAryTreeLearner(partitioner, max_states, max_query_length, max_secs, generate_partial_hipothesis = False, pre_cache_queries_for_building_hipothesis = False,  check_probabilistic_hipothesis = False)\n",
    "#learning_result = learner2.learn(teacher2)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pythautomata.automata.wheighted_automaton_definition.probabilistic_deterministic_finite_automaton.ProbabilisticDeterministicFiniteAutomaton at 0x7f992741dc70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_result.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and submit \n",
    "This is the creation of the model needed for the submission to the competition: you just have to run this cell. It will create in your current directory an **archive** that you can then submit on the competition website.\n",
    "\n",
    "**You should NOT modify this part, just run it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_pdfa_wrapper import MlflowFastPDFA\n",
    "from submit_tools_fix import save_function\n",
    "from fast_pdfa_converter import FastProbabilisticDeterministicFiniteAutomatonConverter as FastPDFAConverter\n",
    "\n",
    "fast_pdfa = FastPDFAConverter().to_fast_pdfa(learning_result.model)\n",
    "mlflow_fast_pdfa = MlflowFastPDFA(fast_pdfa)\n",
    "#save_function(mlflow_fast_pdfa, len(learning_result.model.alphabet), target_model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.937393605485925e-05"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_pdfa.sequence_probability([1,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission created at predicted_models/TEST_PDFA_0.zip.\n"
     ]
    }
   ],
   "source": [
    "from fast_pdfa_wrapper import MlflowFastPDFA\n",
    "from submit_tools_fix import save_function\n",
    "from fast_pdfa_converter import FastProbabilisticDeterministicFiniteAutomatonConverter as FastPDFAConverter\n",
    "\n",
    "fast_pdfa = FastPDFAConverter().to_fast_pdfa(testPDFA)\n",
    "mlflow_fast_pdfa = MlflowFastPDFA(fast_pdfa)\n",
    "save_function(mlflow_fast_pdfa, len(testPDFA.alphabet), \"TEST_PDFA_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission created at predicted_models/TEST_PDFA.zip.\n"
     ]
    }
   ],
   "source": [
    "save_function(mlflow_fast_pdfa, len(testPDFA.alphabet), \"TEST_PDFA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 19, 8, 18, 7, 19, 11, 3, 18, 11, 2, 16, 7, 9, 8, 6, 15, 7, 9, 16, 2, 8, 15, 6, 13, 14, 19, 8, 8, 7, 15, 3, 9, 6, 10, 6, 17, 15, 21], [20, 7, 13, 12, 6, 8, 13, 9, 8, 3, 16, 7, 12, 13, 14, 7, 7, 13, 16, 15, 9, 13, 19, 19, 7, 2, 4, 2, 15, 15, 12, 8, 7, 11, 7, 17, 15, 21], [20, 11, 13, 19, 18, 11, 9, 13, 11, 4, 7, 10, 2, 14, 9, 6, 2, 2, 19, 1, 12, 3, 11, 10, 6, 3, 2, 3, 2, 5, 15, 11, 8, 7, 18, 14, 17, 15, 21], [20, 3, 9, 19, 10, 19, 6, 5, 2, 11, 1, 12, 7, 17, 4, 7, 15, 17, 14, 2, 18, 12, 18, 6, 13, 4, 2, 10, 15, 15, 3, 8, 7, 1, 14, 21], [20, 2, 11, 2, 19, 2, 13, 9, 9, 12, 3, 4, 12, 8, 7, 13, 17, 13, 16, 10, 11, 1, 6, 6, 19, 3, 2, 15, 5, 8, 15, 19, 3, 11, 11, 6, 7, 6, 13, 2, 3, 3, 4, 15, 12, 6, 7, 11, 14, 17, 21], [20, 18, 16, 6, 5, 0, 19, 2, 13, 9, 11, 3, 8, 14, 12, 9, 14, 13, 14, 2, 2, 15, 3, 18, 10, 13, 13, 8, 13, 3, 12, 15, 18, 8, 7, 11, 7, 19, 15, 21], [20, 1, 5, 14, 5, 7, 19, 12, 3, 5, 14, 11, 3, 14, 2, 2, 7, 1, 10, 2, 16, 19, 7, 14, 11, 12, 2, 14, 3, 12, 2, 12, 7, 14, 4, 2, 3, 2, 3, 15, 12, 8, 14, 18, 19, 17, 15, 21], [20, 7, 2, 7, 7, 11, 19, 2, 16, 12, 2, 18, 6, 6, 13, 13, 3, 8, 15, 13, 3, 15, 14, 7, 7, 17, 15, 21], [20, 6, 8, 14, 16, 13, 9, 3, 8, 1, 14, 6, 2, 6, 15, 17, 12, 11, 10, 18, 16, 17, 7, 6, 13, 2, 2, 2, 7, 15, 11, 2, 7, 11, 14, 17, 15, 21], [20, 14, 12, 4, 7, 11, 19, 2, 16, 15, 3, 11, 10, 6, 4, 13, 13, 10, 1, 15, 3, 10, 7, 18, 6, 21]]\n",
      "Model loaded, testing it on 10 sequences\n",
      "5.6972699798664e-51\n",
      "1.196426695771944e-49\n",
      "5.6972699798664e-51\n",
      "5.276241728354273e-47\n",
      "7.745246841429003e-67\n",
      "2.7129857046982854e-52\n",
      "7.172873099847401e-63\n",
      "1.995625488461833e-36\n",
      "1.196426695771944e-49\n",
      "8.800708404116685e-34\n"
     ]
    }
   ],
   "source": [
    "#zip_path = f\"predicted_models/{target_model.name}.zip\"\n",
    "zip_path = f\"predicted_models/TEST_PDFA.zip\"\n",
    "from load_func import load_function\n",
    "print(sequences[0:10])\n",
    "load_function(zip_path, sequences[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NXdKde0kt3FR",
    "eKzzh3hot9vZ",
    "BMQF46fnw1Zk"
   ],
   "name": "PFA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "122e9f251b80a4a76e7262659287020d96f7188da42b39e3d812967db6c8742d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
