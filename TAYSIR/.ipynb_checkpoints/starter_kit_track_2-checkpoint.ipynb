{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAYSIR competition - Track 2 Starter Kit\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This is a notebook to show the structure of a code to participate to the competition.\n",
    "\n",
    "You can also check the baseline notebook (available in the same archive) for more details about the TAYSIR models and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q mlflow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version : 2.0.0+cu117\n",
      "MLflow version : 2.2.2\n",
      "Your python version: 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "print('PyTorch version :', torch.__version__)\n",
    "print('MLflow version :', mlflow.__version__)\n",
    "import sys\n",
    "print(\"Your python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested with:\n",
    "* Torch version: 1.11.0+cu102\n",
    "* MLFlow version: 1.25.1\n",
    "* Python version: 3.8.10 [GCC 9.4.0]\n",
    "\n",
    "Python versions starting at 3.7 are supposed to work (but have not been tested).\n",
    "## Choosing the phase\n",
    "\n",
    "First you must select one of the phases/datasets we provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = 2 #always for this track\n",
    "DATASET = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/26 02:13:27 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cu117'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TNetwork(\n",
       "  23, 22, n_layers=2, neurons_per_layer=64, batch_size=64, patience=5, split_dense=True, task=lm\n",
       "  (mach[0]): RNN(22, 64, batch_first=True)\n",
       "  (mach[1]): RNN(64, 64, batch_first=True)\n",
       "  (dense): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): Linear(in_features=32, out_features=22, bias=True)\n",
       "    (2): Sigmoid()\n",
       "    (3): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = f\"models/2.{DATASET}.taysir.model\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is RNN\n"
     ]
    }
   ],
   "source": [
    "try:#RNN\n",
    "    nb_letters = model.input_size -1\n",
    "    cell_type = model.cell_type\n",
    "\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The type of the recurrent cells is\", cell_type.__name__)\n",
    "except:\n",
    "    nb_letters = model.distilbert.config.vocab_size\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The model is a transformer (DistilBertForSequenceClassification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The input data is in the following format :\n",
    "\n",
    "```\n",
    "[Number of sequences] [Alphabet size]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "...\n",
    "[Length of sequence] [List of symbols]\n",
    "```\n",
    "\n",
    "For example the following data :\n",
    "\n",
    "```\n",
    "5 10\n",
    "6 8 6 5 1 6 7 4 9\n",
    "12 8 6 9 4 6 8 2 1 0 6 5 9\n",
    "7 8 9 4 3 0 4 9\n",
    "4 8 0 4 9\n",
    "8 8 1 5 2 6 0 5 3 9\n",
    "```\n",
    "\n",
    "is composed of 5 sequences and has an alphabet size of 10 (so symbols are between 0 and 9) and the first sequence is composed of 6 symbols (8 6 5 1 6 7 4 9), notice that 8 is the start symbol and 9 is the end symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"datasets/2.{DATASET}.taysir.valid.words\"\n",
    "\n",
    "sequences = []\n",
    "with open(file) as f:\n",
    "    f.readline() #Skip first line (number of sequences, alphabet size)\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        seq = line.split(' ')\n",
    "        seq = [int(i) for i in seq[1:]] #Remove first value (length of sequence) and cast to int\n",
    "        sequences.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *sequences* is thus **a list of lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 9090\n",
      "10 first sequences:\n",
      "[20, 13, 14, 6, 0, 15, 4, 3, 5, 12, 13, 13, 14, 4, 12, 17, 21]\n",
      "[20, 3, 13, 3, 16, 6, 4, 13, 1, 21]\n",
      "[20, 13, 6, 15, 21]\n",
      "[20, 13, 10, 3, 21]\n",
      "[20, 13, 10, 3, 16, 6, 4, 13, 13, 12, 17, 4, 13, 14, 10, 0, 10, 13, 14, 4, 15, 12, 17, 21]\n",
      "[20, 3, 5, 0, 1, 4, 13, 6, 14, 4, 14, 4, 14, 13, 10, 12, 1, 5, 10, 3, 14, 5, 12, 14, 1, 12, 11, 12, 17, 18, 8, 21]\n",
      "[20, 3, 13, 3, 19, 1, 4, 3, 5, 10, 3, 19, 8, 21]\n",
      "[20, 13, 0, 1, 3, 1, 13, 3, 16, 6, 4, 13, 1, 12, 8, 0, 5, 10, 14, 12, 10, 3, 14, 1, 21]\n",
      "[20, 13, 14, 14, 6, 3, 21]\n",
      "[20, 13, 12, 13, 3, 16, 3, 16, 21]\n"
     ]
    }
   ],
   "source": [
    "print('Number of sequences:', len(sequences))\n",
    "print('10 first sequences:')\n",
    "for i in range(10):\n",
    "    print(sequences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model extraction\n",
    "\n",
    "This is where you will extract your simple own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.model_comparators.wfa_partition_comparison_strategy import WFAPartitionComparator\n",
    "from pythautomata.utilities.probability_partitioner import TopKProbabilityPartitioner, QuantizationProbabilityPartitioner, RankingPartitioner\n",
    "from utilities.tata_box_partitioner import TataBoxProbabilityPartitioner\n",
    "#from pythautomata.model_exporters.wfa_image_exporter_with_partition_mapper import WFAImageExporterWithPartitionMapper\n",
    "from pythautomata.base_types.symbol import SymbolStr\n",
    "\n",
    "from pymodelextractor.learners.observation_tree_learners.bounded_pdfa_quantization_n_ary_tree_learner import BoundedPDFAQuantizationNAryTreeLearner\n",
    "from pymodelextractor.teachers.pac_batch_probabilistic_teacher import PACBatchProbabilisticTeacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymodelextractor.teachers.pac_comparison_strategy import PACComparisonStrategy\n",
    "from pymodelextractor.teachers.general_teacher import GeneralTeacher\n",
    "from pymodelextractor.factories.lstar_factory import LStarFactory\n",
    "from pythautomata.utilities.uniform_length_sequence_generator import UniformLengthSequenceGenerator\n",
    "from pymodelextractor.learners.observation_tree_learners.kearns_vazirani_learner import KearnsVaziraniLearner\n",
    "from utils import predict, PytorchLanguageModel\n",
    "\n",
    "name = \"Track: \" + str(TRACK) + \" - DataSet: \" + str(DATASET)\n",
    "\n",
    "target_model = PytorchInference(alphabet, model, name)\n",
    "\n",
    "sequence_generator = UniformLengthSequenceGenerator(alphabet, max_seq_length=1000, min_seq_length=900)\n",
    "\n",
    "comparator = PACComparisonStrategy(target_model_alphabet = alphabet, epsilon = 0.01, delta = 0.01, \n",
    "                                   sequence_generator = sequence_generator)\n",
    "\n",
    "teacher = GeneralTeacher(target_model, comparator)\n",
    "\n",
    "# Choose algorithm (LStar, KV)\n",
    "algorithm = 'LStar'\n",
    "\n",
    "# LStar learner\n",
    "if(algorithm == 'LStar'): learner = LStarFactory.get_dfa_lstar_learner(max_time=5)\n",
    "\n",
    "#Kearns Vazirani learner\n",
    "if(algorithm == 'KV'):  learner = KearnsVaziraniLearner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Once you are satisfied with your model performance, you must write a predict function that takes a sequence as input (list of integers) and returns 0 or 1 (integer type) for the binary classification track and a probability (float type) for the language modeling track.\n",
    "\n",
    "Your model is **NOT** a parameter of this function. You should NOT take care of MLFlow saving here. \n",
    "\n",
    "*For instance, if you want to submit the original model:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sequence):\n",
    "    \"\"\" Define the function that takes a sequence as a list of integers and return the decision of your extracted model. \n",
    "    The function does not need your model as argument. See baseline notebooks for examples.\"\"\"\n",
    "    \n",
    "    #For instance, if you want to submit the original model:\n",
    "    try: #RNN\n",
    "        value = model.predict(model.one_hot_encode(sequence)) \n",
    "        return value\n",
    "    except: #Transformer\n",
    "        def make_future_masks(words:torch.Tensor):\n",
    "            masks = (words != 0)\n",
    "            b,l = masks.size()\n",
    "            #x = einops.einsum(masks, masks, \"b i, b j -> b i j\")\n",
    "            x = torch.einsum(\"bi,bj->bij\",masks,masks)\n",
    "            x *= torch.ones(l,l, dtype=torch.bool, device=x.device).tril()\n",
    "            x += torch.eye(l,dtype=torch.bool, device=x.device)\n",
    "            return x.type(torch.int8)\n",
    "        import numpy\n",
    "        def predict_next_symbols(model, word):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                whole word (list): a complete sequence as a list of integers\n",
    "            Returns:\n",
    "                the predicted probabilities of the next ids for all prefixes (2-D ndarray)\n",
    "            \"\"\"\n",
    "            word = [ [ a+1 for a in word ] ]\n",
    "            word = torch.IntTensor(word)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                attention_mask = make_future_masks(word)\n",
    "                out = model.forward(word, attention_mask=attention_mask)\n",
    "                out = torch.nn.functional.softmax(out.logits[0], dim=1)\n",
    "                return out.detach().numpy()[:, 1:] #  the probabilities for padding id (0) are removed\n",
    "        def predict_transformer(model, word):\n",
    "            probs = predict_next_symbols(model, word[:-1])\n",
    "            probas_for_word = [probs[i,a] for i,a in enumerate(word[1:])]\n",
    "            value = numpy.array(probas_for_word).prod()\n",
    "            return float(value)\n",
    "        return predict_transformer(model, sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and submit \n",
    "This is the creation of the model needed for the submission to the competition: you just have to run this cell. It will create in your current directory an **archive** that you can then submit on the competition website.\n",
    "\n",
    "**You should NOT modify this part, just run it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit_tools import save_function\n",
    "\n",
    "save_function(predict, alphabet_size=nb_letters, prefix=f'dataset_{TRACK}.{DATASET}_')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NXdKde0kt3FR",
    "eKzzh3hot9vZ",
    "BMQF46fnw1Zk"
   ],
   "name": "PFA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "122e9f251b80a4a76e7262659287020d96f7188da42b39e3d812967db6c8742d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
