{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAYSIR competition - Track 2 Example Extraction\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This is a notebook to show the structure of a code to participate to the competition.\n",
    "\n",
    "You can also check the baseline notebook (available in the same archive) for more details about the TAYSIR models and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mlflow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version : 2.0.0+cpu\n",
      "MLflow version : 2.2.2\n",
      "Your python version: 3.9.1 (default, Dec 11 2020, 09:29:25) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "print('PyTorch version :', torch.__version__)\n",
    "print('MLflow version :', mlflow.__version__)\n",
    "import sys\n",
    "print(\"Your python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested with:\n",
    "* Torch version: 1.11.0+cu102\n",
    "* MLFlow version: 1.25.1\n",
    "* Python version: 3.8.10 [GCC 9.4.0]\n",
    "\n",
    "Python versions starting at 3.7 are supposed to work (but have not been tested).\n",
    "## Choosing the phase\n",
    "\n",
    "First you must select one of the phases/datasets we provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/27 13:50:31 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/03/27 13:50:31 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/03/27 13:50:31 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/03/27 13:50:32 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/03/27 13:50:32 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model: 1\n",
      "TNetwork(\n",
      "  23, 22, n_layers=2, neurons_per_layer=64, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): RNN(22, 64, batch_first=True)\n",
      "  (mach[1]): RNN(64, 64, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=22, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is RNN\n",
      "\n",
      "\n",
      "Model: 2\n",
      "TNetwork(\n",
      "  10, 9, n_layers=2, neurons_per_layer=256, cell_type=lstmx.LSTMx, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): LSTMx(\n",
      "    9, 256, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(9, 256)\n",
      "  )\n",
      "  (mach[1]): LSTMx(\n",
      "    256, 256, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(256, 256)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=9, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 9 symbols.\n",
      "The type of the recurrent cells is LSTMx\n",
      "\n",
      "\n",
      "Model: 3\n",
      "TNetwork(\n",
      "  18, 17, n_layers=2, neurons_per_layer=64, cell_type=lstmx.LSTMx, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): LSTMx(\n",
      "    17, 64, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(17, 64)\n",
      "  )\n",
      "  (mach[1]): LSTMx(\n",
      "    64, 64, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(64, 64)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=17, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 17 symbols.\n",
      "The type of the recurrent cells is LSTMx\n",
      "\n",
      "\n",
      "Model: 4\n",
      "TNetwork(\n",
      "  23, 22, cell_type=torch.nn.modules.rnn.GRU, batch_size=128, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): GRU(22, 32, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=22, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is GRU\n",
      "\n",
      "\n",
      "Model: 5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/27 13:50:32 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/03/27 13:50:32 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/03/27 13:50:32 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/03/27 13:50:32 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/03/27 13:50:32 WARNING mlflow.pytorch: Stored model version '1.11.0+cu102' does not match installed PyTorch version '2.0.0+cpu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TNetwork(\n",
      "  36, 35, neurons_per_layer=512, cell_type=lstmx.LSTMx, batch_size=64, patience=20, split_dense=True, task=lm\n",
      "  (mach[0]): LSTMx(\n",
      "    35, 512, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(35, 512)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=35, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 35 symbols.\n",
      "The type of the recurrent cells is LSTMx\n",
      "\n",
      "\n",
      "Model: 6\n",
      "TNetwork(\n",
      "  23, 22, cell_type=torch.nn.modules.rnn.GRU, batch_size=64, patience=5, bidirectional=True, task=lm\n",
      "  (mach[0]): GRU(22, 32, batch_first=True, bidirectional=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=22, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is GRU\n",
      "\n",
      "\n",
      "Model: 7\n",
      "TNetwork(\n",
      "  23, 22, n_layers=2, neurons_per_layer=64, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): RNN(22, 64, batch_first=True)\n",
      "  (mach[1]): RNN(64, 64, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=22, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is RNN\n",
      "\n",
      "\n",
      "Model: 8\n",
      "TNetwork(\n",
      "  27, 26, n_layers=2, neurons_per_layer=124, batch_size=64, patience=5, task=lm\n",
      "  (mach[0]): RNN(26, 124, batch_first=True)\n",
      "  (mach[1]): RNN(124, 124, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=124, out_features=26, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 26 symbols.\n",
      "The type of the recurrent cells is RNN\n",
      "\n",
      "\n",
      "Model: 9\n",
      "TNetwork(\n",
      "  23, 22, cell_type=torch.nn.modules.rnn.GRU, batch_size=64, patience=5, bidirectional=True, task=lm\n",
      "  (mach[0]): GRU(22, 32, batch_first=True, bidirectional=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=22, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is GRU\n",
      "\n",
      "\n",
      "Model: 10\n",
      "DistilBertForTokenClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(64, 256, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 256)\n",
      "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-7): 8 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (lin2): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=256, out_features=64, bias=True)\n",
      ")\n",
      "The alphabet contains 35 symbols.\n",
      "The model is a transformer (DistilBertForSequenceClassification)\n"
     ]
    }
   ],
   "source": [
    "dataset_amount = 10\n",
    "for ds in range(1,dataset_amount+1):\n",
    "    DATASET = ds\n",
    "    model_name = f\"models/2.{DATASET}.taysir.model\"\n",
    "    model = mlflow.pytorch.load_model(model_name)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Model:\", ds)\n",
    "    print(model.eval())\n",
    "    try:#RNN\n",
    "        nb_letters = model.input_size -1\n",
    "        cell_type = model.cell_type\n",
    "\n",
    "        print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "        print(\"The type of the recurrent cells is\", cell_type.__name__)\n",
    "    except:\n",
    "        nb_letters = model.distilbert.config.vocab_size\n",
    "        print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "        print(\"The model is a transformer (DistilBertForSequenceClassification)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = 1 #always for this track\n",
    "DATASET = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/27 13:50:33 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TNetwork(\n",
       "  10, 9, n_layers=2, neurons_per_layer=256, cell_type=lstmx.LSTMx, batch_size=64, patience=5, split_dense=True, task=lm\n",
       "  (mach[0]): LSTMx(\n",
       "    9, 256, batch_first=True\n",
       "    (drop_layer): Dropout(p=0, inplace=False)\n",
       "    (forward_layers[0]): LSTMCell(9, 256)\n",
       "  )\n",
       "  (mach[1]): LSTMx(\n",
       "    256, 256, batch_first=True\n",
       "    (drop_layer): Dropout(p=0, inplace=False)\n",
       "    (forward_layers[0]): LSTMCell(256, 256)\n",
       "  )\n",
       "  (dense): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=9, bias=True)\n",
       "    (2): Sigmoid()\n",
       "    (3): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = f\"models/2.{DATASET}.taysir.model\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alphabet contains 9 symbols.\n",
      "The type of the recurrent cells is LSTMx\n"
     ]
    }
   ],
   "source": [
    "try:#RNN\n",
    "    nb_letters = model.input_size -1\n",
    "    cell_type = model.cell_type\n",
    "\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The type of the recurrent cells is\", cell_type.__name__)\n",
    "except:\n",
    "    nb_letters = model.distilbert.config.vocab_size\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The model is a transformer (DistilBertForSequenceClassification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The input data is in the following format :\n",
    "\n",
    "```\n",
    "[Number of sequences] [Alphabet size]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "...\n",
    "[Length of sequence] [List of symbols]\n",
    "```\n",
    "\n",
    "For example the following data :\n",
    "\n",
    "```\n",
    "5 10\n",
    "6 8 6 5 1 6 7 4 9\n",
    "12 8 6 9 4 6 8 2 1 0 6 5 9\n",
    "7 8 9 4 3 0 4 9\n",
    "4 8 0 4 9\n",
    "8 8 1 5 2 6 0 5 3 9\n",
    "```\n",
    "\n",
    "is composed of 5 sequences and has an alphabet size of 10 (so symbols are between 0 and 9) and the first sequence is composed of 6 symbols (8 6 5 1 6 7 4 9), notice that 8 is the start symbol and 9 is the end symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.base_types.alphabet import Alphabet\n",
    "\n",
    "file = f\"datasets/2.{DATASET}.taysir.valid.words\"\n",
    "\n",
    "alphabet = None\n",
    "sequences = []\n",
    "\n",
    "#In the competition the empty sequence is defined as [alphabet_size - 2, alphabet size -1]\n",
    "#For example with the alphabet of size 22 the empty sequence is [20, 21]\n",
    "empty_sequence_len = 2\n",
    "\n",
    "with open(file) as f:\n",
    "    a = f.readline() #Skip first line (number of sequences, alphabet size)\n",
    "    headline = a.split(' ')\n",
    "    alphabet_size = int(headline[1].strip())\n",
    "    alphabet = Alphabet.from_strings([str(x) for x in range(alphabet_size - empty_sequence_len)])\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        seq = line.split(' ')\n",
    "        seq = [int(i) for i in seq[1:]] #Remove first value (length of sequence) and cast to int\n",
    "        sequences.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *sequences* is thus **a list of lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 9090\n",
      "10 first sequences:\n",
      "[7, 6, 5, 1, 3, 8]\n",
      "[7, 1, 4, 1, 1, 3, 2, 1, 6, 1, 5, 8]\n",
      "[7, 4, 3, 2, 1, 5, 5, 3, 2, 1, 6, 5, 1, 5, 8]\n",
      "[7, 1, 6, 6, 6, 5, 5, 3, 5, 6, 8]\n",
      "[7, 6, 1, 6, 4, 8]\n",
      "[7, 1, 1, 4, 3, 6, 4, 0, 1, 1, 3, 2, 6, 6, 6, 4, 6, 1, 5, 8]\n",
      "[7, 8]\n",
      "[7, 2, 1, 6, 4, 1, 3, 1, 6, 6, 2, 5, 3, 2, 3, 6, 1, 6, 1, 0, 2, 1, 6, 5, 1, 0, 5, 1, 8]\n",
      "[7, 8]\n",
      "[7, 8]\n"
     ]
    }
   ],
   "source": [
    "print('Number of sequences:', len(sequences))\n",
    "print('10 first sequences:')\n",
    "for i in range(10):\n",
    "    print(sequences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def their_predict(sequence, model):\n",
    "    \"\"\" Define the function that takes a sequence as a list of integers and return the decision of your extracted model. \n",
    "    The function does not need your model as argument. See baseline notebooks for examples.\"\"\"\n",
    "    \n",
    "    #For instance, if you want to submit the original model:\n",
    "    try: #RNN\n",
    "        value = model.predict(model.one_hot_encode(sequence)) \n",
    "        return value\n",
    "    except: #Transformer\n",
    "        def make_future_masks(words:torch.Tensor):\n",
    "            masks = (words != 0)\n",
    "            b,l = masks.size()\n",
    "            #x = einops.einsum(masks, masks, \"b i, b j -> b i j\")\n",
    "            x = torch.einsum(\"bi,bj->bij\",masks,masks)\n",
    "            x *= torch.ones(l,l, dtype=torch.bool, device=x.device).tril()\n",
    "            x += torch.eye(l,dtype=torch.bool, device=x.device)\n",
    "            return x.type(torch.int8)\n",
    "        import numpy\n",
    "        def predict_next_symbols(model, word):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                whole word (list): a complete sequence as a list of integers\n",
    "            Returns:\n",
    "                the predicted probabilities of the next ids for all prefixes (2-D ndarray)\n",
    "            \"\"\"\n",
    "            word = [ [ a+1 for a in word ] ]\n",
    "            word = torch.IntTensor(word)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                attention_mask = make_future_masks(word)\n",
    "                out = model.forward(word, attention_mask=attention_mask)\n",
    "                out = torch.nn.functional.softmax(out.logits[0], dim=1)\n",
    "                return out.detach().numpy()[:, 1:] #  the probabilities for padding id (0) are removed\n",
    "        def predict_transformer(model, word):\n",
    "            probs = predict_next_symbols(model, word[:-1])\n",
    "            probas_for_word = [probs[i,a] for i,a in enumerate(word[1:])]\n",
    "            value = numpy.array(probas_for_word).prod()\n",
    "            return float(value)\n",
    "        return predict_transformer(model, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model extraction\n",
    "\n",
    "This is where you will extract your simple own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.model_comparators.wfa_partition_comparison_strategy import WFAPartitionComparator\n",
    "from pythautomata.utilities.probability_partitioner import QuantizationProbabilityPartitioner\n",
    "#from pythautomata.model_exporters.wfa_image_exporter_with_partition_mapper import WFAImageExporterWithPartitionMapper\n",
    "from pythautomata.base_types.symbol import SymbolStr\n",
    "from pythautomata.utilities.uniform_length_sequence_generator import UniformLengthSequenceGenerator\n",
    "\n",
    "from pymodelextractor.learners.observation_tree_learners.bounded_pdfa_quantization_n_ary_tree_learner import BoundedPDFAQuantizationNAryTreeLearner\n",
    "from pymodelextractor.teachers.pac_batch_probabilistic_teacher import PACBatchProbabilisticTeacher\n",
    "from pymodelextractor.teachers.pac_probabilistic_teacher import PACProbabilisticTeacher\n",
    "\n",
    "from utils import predict\n",
    "from pytorch_language_model import PytorchLanguageModel\n",
    "\n",
    "name = \"track_\" + str(2) + \"_dataset_\" + str(2)\n",
    "\n",
    "target_model = PytorchLanguageModel(alphabet, model, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:30\u001b[0m, in \u001b[0;36mfull_next_symbols_probas\u001b[1;34m(sequence, model)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m: \u001b[39m#RNN\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     value, hiddens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward_lm(model\u001b[39m.\u001b[39;49mone_hot_encode(sequence))\n\u001b[0;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m value\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.2.taysir.model\\code\\tnetwork.py:178\u001b[0m, in \u001b[0;36mTNetwork.forward_lm\u001b[1;34m(self, x, hidden, full_ret)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)]:\n\u001b[1;32m--> 178\u001b[0m     ohx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_bin(word[:i], h_0, full_ret)\n\u001b[0;32m    179\u001b[0m     o, h \u001b[39m=\u001b[39m ohx[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.2.taysir.model\\code\\tnetwork.py:144\u001b[0m, in \u001b[0;36mTNetwork.forward_bin\u001b[1;34m(self, x, hidden, full_ret)\u001b[0m\n\u001b[0;32m    143\u001b[0m words \u001b[39m=\u001b[39m unpad(unbatch(x))\n\u001b[1;32m--> 144\u001b[0m states \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pass_recurrent(word, hidden, b\u001b[39m=\u001b[39mb)\n\u001b[0;32m    145\u001b[0m           \u001b[39mfor\u001b[39;00m b,word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words)]\n\u001b[0;32m    146\u001b[0m outs, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_states(states)\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.2.taysir.model\\code\\tnetwork.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    143\u001b[0m words \u001b[39m=\u001b[39m unpad(unbatch(x))\n\u001b[1;32m--> 144\u001b[0m states \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pass_recurrent(word, hidden, b\u001b[39m=\u001b[39;49mb)\n\u001b[0;32m    145\u001b[0m           \u001b[39mfor\u001b[39;00m b,word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words)]\n\u001b[0;32m    146\u001b[0m outs, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_states(states)\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.2.taysir.model\\code\\tnetwork.py:324\u001b[0m, in \u001b[0;36mTNetwork._pass_recurrent\u001b[1;34m(self, word, hidden, b)\u001b[0m\n\u001b[0;32m    323\u001b[0m         h \u001b[39m=\u001b[39m h[:,b]\n\u001b[1;32m--> 324\u001b[0m out \u001b[39m=\u001b[39m machine(out[\u001b[39m0\u001b[39m], h, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.2.taysir.model\\code\\lstmx.py:284\u001b[0m, in \u001b[0;36mLSTMx.forward\u001b[1;34m(self, x, hxcx, full_ret)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, hxcx\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, full_ret\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 284\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_x(x, hxcx)\n\u001b[0;32m    285\u001b[0m     \u001b[39mif\u001b[39;00m full_ret:\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.2.taysir.model\\code\\lstmx.py:352\u001b[0m, in \u001b[0;36mLSTMx.forward_x\u001b[1;34m(self, x, hxcx)\u001b[0m\n\u001b[0;32m    351\u001b[0m     batched \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_layer(batched)\n\u001b[1;32m--> 352\u001b[0m h, c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_layer(batched, hx[:,i], cx[:,i], i)\n\u001b[0;32m    353\u001b[0m batched \u001b[39m=\u001b[39m h\n",
      "File \u001b[1;32m~\\Repos\\Taysir_Competition\\TAYSIR\\models\\2.2.taysir.model\\code\\lstmx.py:391\u001b[0m, in \u001b[0;36mLSTMx._run_layer\u001b[1;34m(self, x, hx, cx, t)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m timesteps:\n\u001b[1;32m--> 391\u001b[0m     h, c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_layers[t](element, (h, c))\n\u001b[0;32m    392\u001b[0m     h_out\u001b[39m.\u001b[39mappend(h)\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1235\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1235\u001b[0m     hx \u001b[39m=\u001b[39m (hx[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched \u001b[39melse\u001b[39;00m hx\n\u001b[0;32m   1237\u001b[0m ret \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm_cell(\n\u001b[0;32m   1238\u001b[0m     \u001b[39minput\u001b[39m, hx,\n\u001b[0;32m   1239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_ih, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_hh,\n\u001b[0;32m   1240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias_ih, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias_hh,\n\u001b[0;32m   1241\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pdfa_extractor.ipynb Cell 20\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/pdfa_extractor.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m teacher  \u001b[39m=\u001b[39m PACProbabilisticTeacher(target_model, epsilon \u001b[39m=\u001b[39m epsilon, delta \u001b[39m=\u001b[39m delta, max_seq_length \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, comparator \u001b[39m=\u001b[39m comparator, sequence_generator\u001b[39m=\u001b[39msequence_generator, compute_epsilon_star\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/pdfa_extractor.ipynb#X40sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m learner \u001b[39m=\u001b[39m BoundedPDFAQuantizationNAryTreeLearner(partitioner, max_states, max_query_length, max_secs, generate_partial_hipothesis \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, pre_cache_queries_for_building_hipothesis \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,  check_probabilistic_hipothesis \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/pdfa_extractor.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m learning_result \u001b[39m=\u001b[39m learner\u001b[39m.\u001b[39;49mlearn(teacher)\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\bounded_pdfa_quantization_n_ary_tree_learner.py:48\u001b[0m, in \u001b[0;36mBoundedPDFAQuantizationNAryTreeLearner.learn\u001b[1;34m(self, teacher, verbose)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_learning_with_time_bound(teacher, verbose)\n\u001b[0;32m     47\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m         \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(teacher, verbose)\n\u001b[0;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m NumberOfStatesExceededException:\n\u001b[0;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumberOfStatesExceeded\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\pdfa_quantization_n_ary_tree_learner.py:82\u001b[0m, in \u001b[0;36mPDFAQuantizationNAryTreeLearner.learn\u001b[1;34m(self, teacher, verbose)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mfor\u001b[39;00m symbol \u001b[39min\u001b[39;00m symbols:\n\u001b[0;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tree\u001b[39m.\u001b[39msift(Sequence([symbol]))\n\u001b[1;32m---> 82\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtentative_hypothesis()\n\u001b[0;32m     83\u001b[0m models\u001b[39m.\u001b[39mappend(model)\n\u001b[0;32m     84\u001b[0m last_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mweighted_states)\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\pdfa_quantization_n_ary_tree_learner.py:143\u001b[0m, in \u001b[0;36mPDFAQuantizationNAryTreeLearner.tentative_hypothesis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m access_string, state \u001b[39min\u001b[39;00m states\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    142\u001b[0m     \u001b[39mfor\u001b[39;00m symbol \u001b[39min\u001b[39;00m symbols:\n\u001b[1;32m--> 143\u001b[0m         access_string_of_transition, updated_tree \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tree\u001b[39m.\u001b[39;49msift(access_string \u001b[39m+\u001b[39;49m symbol)\n\u001b[0;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m updated_tree:\n\u001b[0;32m    145\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\pdfa_quantization_n_ary_tree_learner.py:244\u001b[0m, in \u001b[0;36mClassificationTree.sift\u001b[1;34m(self, sequence, update)\u001b[0m\n\u001b[0;32m    242\u001b[0m d \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mstring\n\u001b[0;32m    243\u001b[0m sd \u001b[39m=\u001b[39m sequence \u001b[39m+\u001b[39m d\n\u001b[1;32m--> 244\u001b[0m sd_probabilities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_token_probabilities(sd, update)\u001b[39m.\u001b[39mvalues()\n\u001b[0;32m    245\u001b[0m child_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_look_for_branch(node\u001b[39m.\u001b[39mchilds, \u001b[39mlist\u001b[39m(sd_probabilities))\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m child_key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\pdfa_quantization_n_ary_tree_learner.py:276\u001b[0m, in \u001b[0;36mClassificationTree._next_token_probabilities\u001b[1;34m(self, sequence, check_max_query_length)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_token_probabilities_cache[sequence]\n\u001b[0;32m    275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_teacher\u001b[39m.\u001b[39;49mnext_token_probabilities(sequence)\n\u001b[0;32m    277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_token_probabilities_cache[sequence] \u001b[39m=\u001b[39m value\n\u001b[0;32m    278\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\teachers\\probabilistic_teacher.py:37\u001b[0m, in \u001b[0;36mProbabilisticTeacher.next_token_probabilities\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     35\u001b[0m symbols\u001b[39m.\u001b[39msort()\n\u001b[0;32m     36\u001b[0m symbols \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mterminal_symbol] \u001b[39m+\u001b[39m symbols\n\u001b[1;32m---> 37\u001b[0m probabilities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_token_weights(sequence, symbols)\n\u001b[0;32m     38\u001b[0m probabilities \u001b[39m=\u001b[39m OrderedDict(\u001b[39mzip\u001b[39m(symbols, probabilities))\n\u001b[0;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m probabilities\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\teachers\\pac_probabilistic_teacher.py:40\u001b[0m, in \u001b[0;36mPACProbabilisticTeacher.last_token_weights\u001b[1;34m(self, sequence, required_suffixes)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlast_token_weights\u001b[39m(\u001b[39mself\u001b[39m, sequence: Sequence, required_suffixes: \u001b[39mlist\u001b[39m[Sequence]):\n\u001b[0;32m     39\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_token_weight_queries_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(required_suffixes)        \n\u001b[1;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_target_model\u001b[39m.\u001b[39;49mget_last_token_weights(sequence, required_suffixes)\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pytorch_language_model.py:96\u001b[0m, in \u001b[0;36mPytorchLanguageModel.get_last_token_weights\u001b[1;34m(self, sequence, required_suffixes)\u001b[0m\n\u001b[0;32m     94\u001b[0m         new_prefix \u001b[39m=\u001b[39m Sequence(new_sequence[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     95\u001b[0m         new_suffix \u001b[39m=\u001b[39m new_sequence[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 96\u001b[0m         next_symbol_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_symbol_probas(new_prefix)\n\u001b[0;32m     97\u001b[0m         weights\u001b[39m.\u001b[39mappend(next_symbol_weights[new_suffix])\n\u001b[0;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m weights\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pytorch_language_model.py:56\u001b[0m, in \u001b[0;36mPytorchLanguageModel.next_symbol_probas\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext_symbol_probas\u001b[39m(\u001b[39mself\u001b[39m, sequence: Sequence):\n\u001b[0;32m     52\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    Function that returns a dictionary with the probability of next symbols (not including padding_symbol)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m    Quickly implemented, depends on raw_next_symbol_probas(sequence) \u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m                \n\u001b[1;32m---> 56\u001b[0m     next_probas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_next_symbol_probas(sequence)\n\u001b[0;32m     58\u001b[0m     symbols \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphabet\u001b[39m.\u001b[39msymbols) \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_terminal_symbol]\n\u001b[0;32m     59\u001b[0m     intermediate_dict \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pytorch_language_model.py:45\u001b[0m, in \u001b[0;36mPytorchLanguageModel.raw_next_symbol_probas\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraw_next_symbol_probas\u001b[39m(\u001b[39mself\u001b[39m, sequence: Sequence):\n\u001b[1;32m---> 45\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_query(sequence)        \n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pytorch_language_model.py:34\u001b[0m, in \u001b[0;36mPytorchLanguageModel.process_query\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_query\u001b[39m(\u001b[39mself\u001b[39m, sequence):\n\u001b[0;32m     33\u001b[0m     adapted_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapt_sequence(sequence)\n\u001b[1;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mnext_symbols_probas(adapted_sequence, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model)\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:59\u001b[0m, in \u001b[0;36mnext_symbols_probas\u001b[1;34m(sequence, model)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext_symbols_probas\u001b[39m(sequence, model):   \n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m full_next_symbols_probas(sequence, model)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:56\u001b[0m, in \u001b[0;36mfull_next_symbols_probas\u001b[1;34m(sequence, model)\u001b[0m\n\u001b[0;32m     54\u001b[0m         out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(out\u001b[39m.\u001b[39mlogits[\u001b[39m0\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m         \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()[:, \u001b[39m1\u001b[39m:] \u001b[39m#  the probabilities for padding id (0) are removed        \u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m predict_next_symbols(model, sequence[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:53\u001b[0m, in \u001b[0;36mfull_next_symbols_probas.<locals>.predict_next_symbols\u001b[1;34m(model, word)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     52\u001b[0m     attention_mask \u001b[39m=\u001b[39m make_future_masks(word)\n\u001b[1;32m---> 53\u001b[0m     out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(word, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m     54\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(out\u001b[39m.\u001b[39mlogits[\u001b[39m0\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()[:, \u001b[39m1\u001b[39m:]\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'attention_mask'"
     ]
    }
   ],
   "source": [
    "epsilon = 0.1\n",
    "delta = 0.1\n",
    "max_states = 1000000\n",
    "max_query_length= 1000000\n",
    "max_secs = None\n",
    "sequence_generator = UniformLengthSequenceGenerator(alphabet, max_seq_length=100, min_seq_length=20)\n",
    "\n",
    "partitioner = QuantizationProbabilityPartitioner(10)\n",
    "comparator = WFAPartitionComparator(partitioner)\n",
    "teacher  = PACProbabilisticTeacher(target_model, epsilon = epsilon, delta = delta, max_seq_length = None, comparator = comparator, sequence_generator=sequence_generator, compute_epsilon_star=False)\n",
    "learner = BoundedPDFAQuantizationNAryTreeLearner(partitioner, max_states, max_query_length, max_secs, generate_partial_hipothesis = False, pre_cache_queries_for_building_hipothesis = False,  check_probabilistic_hipothesis = False)\n",
    "learning_result = learner.learn(teacher)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Once you are satisfied with your model performance, you must write a predict function that takes a sequence as input (list of integers) and returns 0 or 1 (integer type) for the binary classification track and a probability (float type) for the language modeling track.\n",
    "\n",
    "Your model is **NOT** a parameter of this function. You should NOT take care of MLFlow saving here. \n",
    "\n",
    "*For instance, if you want to submit the original model:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sequence):\n",
    "    \"\"\" Define the function that takes a sequence as a list of integers and return the decision of your extracted model. \n",
    "    The function does not need your model as argument. See baseline notebooks for examples.\"\"\"\n",
    "    \n",
    "    #For instance, if you want to submit the original model:\n",
    "    try: #RNN\n",
    "        value = model.predict(model.one_hot_encode(sequence)) \n",
    "        return value\n",
    "    except: #Transformer\n",
    "        def make_future_masks(words:torch.Tensor):\n",
    "            masks = (words != 0)\n",
    "            b,l = masks.size()\n",
    "            #x = einops.einsum(masks, masks, \"b i, b j -> b i j\")\n",
    "            x = torch.einsum(\"bi,bj->bij\",masks,masks)\n",
    "            x *= torch.ones(l,l, dtype=torch.bool, device=x.device).tril()\n",
    "            x += torch.eye(l,dtype=torch.bool, device=x.device)\n",
    "            return x.type(torch.int8)\n",
    "        import numpy\n",
    "        def predict_next_symbols(model, word):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                whole word (list): a complete sequence as a list of integers\n",
    "            Returns:\n",
    "                the predicted probabilities of the next ids for all prefixes (2-D ndarray)\n",
    "            \"\"\"\n",
    "            word = [ [ a+1 for a in word ] ]\n",
    "            word = torch.IntTensor(word)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                attention_mask = make_future_masks(word)\n",
    "                out = model.forward(word, attention_mask=attention_mask)\n",
    "                out = torch.nn.functional.softmax(out.logits[0], dim=1)\n",
    "                return out.detach().numpy()[:, 1:] #  the probabilities for padding id (0) are removed\n",
    "        def predict_transformer(model, word):\n",
    "            probs = predict_next_symbols(model, word[:-1])\n",
    "            probas_for_word = [probs[i,a] for i,a in enumerate(word[1:])]\n",
    "            value = numpy.array(probas_for_word).prod()\n",
    "            return float(value)\n",
    "        return predict_transformer(model, sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and submit \n",
    "This is the creation of the model needed for the submission to the competition: you just have to run this cell. It will create in your current directory an **archive** that you can then submit on the competition website.\n",
    "\n",
    "**You should NOT modify this part, just run it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit_tools import save_function\n",
    "\n",
    "save_function(predict, alphabet_size=nb_letters, prefix=f'dataset_{TRACK}.{DATASET}_')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NXdKde0kt3FR",
    "eKzzh3hot9vZ",
    "BMQF46fnw1Zk"
   ],
   "name": "PFA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "122e9f251b80a4a76e7262659287020d96f7188da42b39e3d812967db6c8742d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
