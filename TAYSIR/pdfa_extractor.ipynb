{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAYSIR competition - Track 2 Example Extraction\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This is a notebook to show the structure of a code to participate to the competition.\n",
    "\n",
    "You can also check the baseline notebook (available in the same archive) for more details about the TAYSIR models and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version : 2.0.0+cpu\n",
      "MLflow version : 2.2.2\n",
      "Your python version: 3.9.1 (default, Dec 11 2020, 09:29:25) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "print('PyTorch version :', torch.__version__)\n",
    "print('MLflow version :', mlflow.__version__)\n",
    "import sys\n",
    "print(\"Your python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested with:\n",
    "* Torch version: 1.11.0+cu102\n",
    "* MLFlow version: 1.25.1\n",
    "* Python version: 3.8.10 [GCC 9.4.0]\n",
    "\n",
    "Python versions starting at 3.7 are supposed to work (but have not been tested).\n",
    "## Choosing the phase\n",
    "\n",
    "First you must select one of the phases/datasets we provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/04 14:22:33 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/04/04 14:22:33 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/04/04 14:22:33 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/04/04 14:22:33 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/04/04 14:22:33 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/04/04 14:22:33 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/04/04 14:22:33 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/04/04 14:22:33 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/04/04 14:22:33 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.0.0+cpu'\n",
      "2023/04/04 14:22:33 WARNING mlflow.pytorch: Stored model version '1.11.0+cu102' does not match installed PyTorch version '2.0.0+cpu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model: 1\n",
      "TNetwork(\n",
      "  23, 22, n_layers=2, neurons_per_layer=64, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): RNN(22, 64, batch_first=True)\n",
      "  (mach[1]): RNN(64, 64, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=22, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is RNN\n",
      "\n",
      "\n",
      "Model: 2\n",
      "TNetwork(\n",
      "  10, 9, n_layers=2, neurons_per_layer=256, cell_type=lstmx.LSTMx, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): LSTMx(\n",
      "    9, 256, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(9, 256)\n",
      "  )\n",
      "  (mach[1]): LSTMx(\n",
      "    256, 256, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(256, 256)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=9, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 9 symbols.\n",
      "The type of the recurrent cells is LSTMx\n",
      "\n",
      "\n",
      "Model: 3\n",
      "TNetwork(\n",
      "  18, 17, n_layers=2, neurons_per_layer=64, cell_type=lstmx.LSTMx, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): LSTMx(\n",
      "    17, 64, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(17, 64)\n",
      "  )\n",
      "  (mach[1]): LSTMx(\n",
      "    64, 64, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(64, 64)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=17, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 17 symbols.\n",
      "The type of the recurrent cells is LSTMx\n",
      "\n",
      "\n",
      "Model: 4\n",
      "TNetwork(\n",
      "  23, 22, cell_type=torch.nn.modules.rnn.GRU, batch_size=128, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): GRU(22, 32, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=22, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is GRU\n",
      "\n",
      "\n",
      "Model: 5\n",
      "TNetwork(\n",
      "  36, 35, neurons_per_layer=512, cell_type=lstmx.LSTMx, batch_size=64, patience=20, split_dense=True, task=lm\n",
      "  (mach[0]): LSTMx(\n",
      "    35, 512, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(35, 512)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=35, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 35 symbols.\n",
      "The type of the recurrent cells is LSTMx\n",
      "\n",
      "\n",
      "Model: 6\n",
      "TNetwork(\n",
      "  23, 22, cell_type=torch.nn.modules.rnn.GRU, batch_size=64, patience=5, bidirectional=True, task=lm\n",
      "  (mach[0]): GRU(22, 32, batch_first=True, bidirectional=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=22, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is GRU\n",
      "\n",
      "\n",
      "Model: 7\n",
      "TNetwork(\n",
      "  23, 22, n_layers=2, neurons_per_layer=64, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): RNN(22, 64, batch_first=True)\n",
      "  (mach[1]): RNN(64, 64, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=22, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is RNN\n",
      "\n",
      "\n",
      "Model: 8\n",
      "TNetwork(\n",
      "  27, 26, n_layers=2, neurons_per_layer=124, batch_size=64, patience=5, task=lm\n",
      "  (mach[0]): RNN(26, 124, batch_first=True)\n",
      "  (mach[1]): RNN(124, 124, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=124, out_features=26, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 26 symbols.\n",
      "The type of the recurrent cells is RNN\n",
      "\n",
      "\n",
      "Model: 9\n",
      "TNetwork(\n",
      "  23, 22, cell_type=torch.nn.modules.rnn.GRU, batch_size=64, patience=5, bidirectional=True, task=lm\n",
      "  (mach[0]): GRU(22, 32, batch_first=True, bidirectional=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=22, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is GRU\n",
      "\n",
      "\n",
      "Model: 10\n",
      "DistilBertForTokenClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(64, 256, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 256)\n",
      "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-7): 8 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (lin2): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=256, out_features=64, bias=True)\n",
      ")\n",
      "The alphabet contains 35 symbols.\n",
      "The model is a transformer (DistilBertForSequenceClassification)\n"
     ]
    }
   ],
   "source": [
    "dataset_amount = 10\n",
    "for ds in range(1,dataset_amount+1):\n",
    "    DATASET = ds\n",
    "    model_name = f\"models/2.{DATASET}.taysir.model\"\n",
    "    model = mlflow.pytorch.load_model(model_name)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Model:\", ds)\n",
    "    print(model.eval())\n",
    "    try:#RNN\n",
    "        nb_letters = model.input_size -1\n",
    "        cell_type = model.cell_type\n",
    "\n",
    "        print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "        print(\"The type of the recurrent cells is\", cell_type.__name__)\n",
    "    except:\n",
    "        nb_letters = model.distilbert.config.vocab_size\n",
    "        print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "        print(\"The model is a transformer (DistilBertForSequenceClassification)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = 2 #always for this track\n",
    "DATASET = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/04 14:22:40 WARNING mlflow.pytorch: Stored model version '1.11.0+cu102' does not match installed PyTorch version '2.0.0+cpu'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForTokenClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(64, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-7): 8 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (lin2): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = f\"models/2.{DATASET}.taysir.model\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alphabet contains 35 symbols.\n",
      "The model is a transformer (DistilBertForSequenceClassification)\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(model, 'distilbert'):#RNN\n",
    "    nb_letters = model.input_size -1\n",
    "    cell_type = model.cell_type\n",
    "\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The type of the recurrent cells is\", cell_type.__name__)\n",
    "else:\n",
    "    nb_letters = model.distilbert.config.vocab_size\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The model is a transformer (DistilBertForSequenceClassification)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The input data is in the following format :\n",
    "\n",
    "```\n",
    "[Number of sequences] [Alphabet size]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "...\n",
    "[Length of sequence] [List of symbols]\n",
    "```\n",
    "\n",
    "For example the following data :\n",
    "\n",
    "```\n",
    "5 10\n",
    "6 8 6 5 1 6 7 4 9\n",
    "12 8 6 9 4 6 8 2 1 0 6 5 9\n",
    "7 8 9 4 3 0 4 9\n",
    "4 8 0 4 9\n",
    "8 8 1 5 2 6 0 5 3 9\n",
    "```\n",
    "\n",
    "is composed of 5 sequences and has an alphabet size of 10 (so symbols are between 0 and 9) and the first sequence is composed of 6 symbols (8 6 5 1 6 7 4 9), notice that 8 is the start symbol and 9 is the end symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.base_types.alphabet import Alphabet\n",
    "\n",
    "file = f\"datasets/2.{DATASET}.taysir.valid.words\"\n",
    "\n",
    "alphabet = None\n",
    "sequences = []\n",
    "\n",
    "#In the competition the empty sequence is defined as [alphabet_size - 2, alphabet size -1]\n",
    "#For example with the alphabet of size 22 the empty sequence is [20, 21]\n",
    "empty_sequence_len = 2\n",
    "\n",
    "with open(file) as f:\n",
    "    a = f.readline() #Skip first line (number of sequences, alphabet size)\n",
    "    headline = a.split(' ')\n",
    "    alphabet_size = int(headline[1].strip())\n",
    "    alphabet = Alphabet.from_strings([str(x) for x in range(alphabet_size - empty_sequence_len)])\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        seq = line.split(' ')\n",
    "        seq = [int(i) for i in seq[1:]] #Remove first value (length of sequence) and cast to int\n",
    "        sequences.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *sequences* is thus **a list of lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 489\n",
      "10 first sequences:\n",
      "[33, 28, 32, 26, 8, 32, 16, 4, 8, 28, 34]\n",
      "[33, 10, 18, 26, 1, 30, 20, 34]\n",
      "[33, 26, 8, 28, 8, 15, 10, 9, 1, 30, 20, 34]\n",
      "[33, 16, 24, 30, 32, 26, 18, 17, 1, 16, 32, 1, 30, 20, 34]\n",
      "[33, 3, 1, 9, 32, 8, 26, 1, 30, 20, 34]\n",
      "[33, 16, 24, 26, 26, 8, 16, 32, 28, 34]\n",
      "[33, 16, 4, 24, 13, 8, 34]\n",
      "[33, 26, 8, 28, 4, 31, 3, 3, 9, 8, 17, 34]\n",
      "[33, 28, 4, 26, 24, 31, 17, 8, 17, 34]\n",
      "[33, 28, 30, 18, 13, 8, 28, 34]\n"
     ]
    }
   ],
   "source": [
    "print('Number of sequences:', len(sequences))\n",
    "print('10 first sequences:')\n",
    "for i in range(10):\n",
    "    print(sequences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model extraction\n",
    "\n",
    "This is where you will extract your simple own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.model_comparators.wfa_partition_comparison_strategy import WFAPartitionComparator\n",
    "from pythautomata.utilities.probability_partitioner import QuantizationProbabilityPartitioner\n",
    "#from pythautomata.model_exporters.wfa_image_exporter_with_partition_mapper import WFAImageExporterWithPartitionMapper\n",
    "from pythautomata.base_types.symbol import SymbolStr\n",
    "from pythautomata.utilities.uniform_length_sequence_generator import UniformLengthSequenceGenerator\n",
    "\n",
    "from pymodelextractor.learners.observation_tree_learners.bounded_pdfa_quantization_n_ary_tree_learner import BoundedPDFAQuantizationNAryTreeLearner\n",
    "from pymodelextractor.teachers.pac_batch_probabilistic_teacher import PACBatchProbabilisticTeacher\n",
    "from pymodelextractor.teachers.pac_probabilistic_teacher import PACProbabilisticTeacher\n",
    "from pymodelextractor.utils.pickle_data_loader import PickleDataLoader\n",
    "\n",
    "from utils import predict\n",
    "from pytorch_language_model import PytorchLanguageModel\n",
    "\n",
    "name = \"track_\" + str(TRACK) + \"_dataset_\" + str(DATASET)\n",
    "\n",
    "target_model = PytorchLanguageModel(alphabet, model, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pdfa_extractor.ipynb Cell 18\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/pdfa_extractor.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlast_token_weights_pickle_dataset_generator\u001b[39;00m \u001b[39mimport\u001b[39;00m LastTokenWeightsPickleDataSetGenerator\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/pdfa_extractor.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m LastTokenWeightsPickleDataSetGenerator()\u001b[39m.\u001b[39;49mgenearte_dataset(target_model, \u001b[39m100\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m./test\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\last_token_weights_pickle_dataset_generator.py:22\u001b[0m, in \u001b[0;36mLastTokenWeightsPickleDataSetGenerator.genearte_dataset\u001b[1;34m(self, model, max_query_elements, path, batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m     queries\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39m(generator))    \n\u001b[0;32m     21\u001b[0m total_elements \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_size                  \n\u001b[1;32m---> 22\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mget_last_token_weights_batch(queries, symbols)                         \n\u001b[0;32m     23\u001b[0m results_od \u001b[39m=\u001b[39m [OrderedDict(\u001b[39mzip\u001b[39m(symbols, x)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m results]\n\u001b[0;32m     24\u001b[0m results  \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(queries, results_od))          \n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pytorch_language_model.py:118\u001b[0m, in \u001b[0;36mPytorchLanguageModel.get_last_token_weights_batch\u001b[1;34m(self, sequences, required_suffixes)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m             seqs_to_query\u001b[39m.\u001b[39madd(seq)\n\u001b[1;32m--> 118\u001b[0m result_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_eval_batch(\u001b[39mlist\u001b[39;49m(seqs_to_query))\n\u001b[0;32m    119\u001b[0m \u001b[39m#result_dict = dict(zip(seqs_to_query, query_results))\u001b[39;00m\n\u001b[0;32m    120\u001b[0m results \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pytorch_language_model.py:157\u001b[0m, in \u001b[0;36mPytorchLanguageModel.raw_eval_batch\u001b[1;34m(self, sequences)\u001b[0m\n\u001b[0;32m    155\u001b[0m     model_evaluation \u001b[39m=\u001b[39m [result[\u001b[39m1\u001b[39m]]\n\u001b[0;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     model_evaluation \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mfull_next_symbols_probas_batch(adapted_sequences_np, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model)[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    158\u001b[0m seqs_to_query\u001b[39m.\u001b[39mextend(seqs)\n\u001b[0;32m    159\u001b[0m query_results\u001b[39m.\u001b[39mextend(model_evaluation)\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:88\u001b[0m, in \u001b[0;36mfull_next_symbols_probas_batch\u001b[1;34m(sequences, model)\u001b[0m\n\u001b[0;32m     86\u001b[0m         out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(out\u001b[39m.\u001b[39mlogits[\u001b[39m0\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     87\u001b[0m         \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()[:,:,\u001b[39m1\u001b[39m:] \u001b[39m#  the probabilities for padding id (0) are removed        \u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[39mreturn\u001b[39;00m predict_next_symbols(model, sequences[:,:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:84\u001b[0m, in \u001b[0;36mfull_next_symbols_probas_batch.<locals>.predict_next_symbols\u001b[1;34m(model, words)\u001b[0m\n\u001b[0;32m     82\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     83\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 84\u001b[0m     attention_mask \u001b[39m=\u001b[39m make_future_masks(words[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     85\u001b[0m     out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(words, attention_mask\u001b[39m=\u001b[39mattention_mask)\n\u001b[0;32m     86\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(out\u001b[39m.\u001b[39mlogits[\u001b[39m0\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\utils.py:67\u001b[0m, in \u001b[0;36mfull_next_symbols_probas_batch.<locals>.make_future_masks\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_future_masks\u001b[39m(words:torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     66\u001b[0m     masks \u001b[39m=\u001b[39m (words \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m     b,l \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39msize()\n\u001b[0;32m     68\u001b[0m     \u001b[39m#x = einops.einsum(masks, masks, \"b i, b j -> b i j\")\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbi,bj->bij\u001b[39m\u001b[39m\"\u001b[39m,masks,masks)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from last_token_weights_pickle_dataset_generator import LastTokenWeightsPickleDataSetGenerator\n",
    "LastTokenWeightsPickleDataSetGenerator().genearte_dataset(target_model, 100, \"./test\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pdfa_extractor.ipynb Cell 18\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/pdfa_extractor.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49m_version__\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute '_version__'"
     ]
    }
   ],
   "source": [
    "sequence_generator = UniformLengthSequenceGenerator(alphabet, max_seq_length=100,\n",
    "                                                        min_seq_length=2)\n",
    "test_sequences = sequence_generator.generate_words(100)\n",
    "[target_model.sequence_probability(x) for x in test_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "timeout only supports UNIX systems for the moment as SIGALRM does not exist on Windows.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franz\\Repos\\Taysir_Competition\\TAYSIR\\pdfa_extractor.ipynb Cell 25\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/pdfa_extractor.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m teacher  \u001b[39m=\u001b[39m PACProbabilisticTeacher(target_model, epsilon \u001b[39m=\u001b[39m epsilon, delta \u001b[39m=\u001b[39m delta, max_seq_length \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, comparator \u001b[39m=\u001b[39m comparator, sequence_generator\u001b[39m=\u001b[39msequence_generator, compute_epsilon_star\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/pdfa_extractor.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m learner \u001b[39m=\u001b[39m BoundedPDFAQuantizationNAryTreeLearner(partitioner, max_states, max_query_length, max_secs, generate_partial_hipothesis \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, pre_cache_queries_for_building_hipothesis \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,  check_probabilistic_hipothesis \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/franz/Repos/Taysir_Competition/TAYSIR/pdfa_extractor.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m learning_result \u001b[39m=\u001b[39m learner\u001b[39m.\u001b[39;49mlearn(teacher)\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\bounded_pdfa_quantization_n_ary_tree_learner.py:46\u001b[0m, in \u001b[0;36mBoundedPDFAQuantizationNAryTreeLearner.learn\u001b[1;34m(self, teacher, verbose)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_seconds_run \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_learning_with_time_bound(teacher, verbose)\n\u001b[0;32m     47\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mlearn(teacher, verbose)\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\learners\\observation_tree_learners\\bounded_pdfa_quantization_n_ary_tree_learner.py:37\u001b[0m, in \u001b[0;36mBoundedPDFAQuantizationNAryTreeLearner.run_learning_with_time_bound\u001b[1;34m(self, teacher, verbose)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_learning_with_time_bound\u001b[39m(\u001b[39mself\u001b[39m, teacher, verbose):\n\u001b[0;32m     36\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 37\u001b[0m         \u001b[39mwith\u001b[39;00m timeout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_seconds_run):\n\u001b[0;32m     38\u001b[0m             \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mlearn(teacher, verbose)\n\u001b[0;32m     39\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\contextlib.py:117\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[0;32m    116\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[0;32m    118\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franz\\anaconda3\\envs\\pymodelextractor_exp\\lib\\site-packages\\pymodelextractor\\utils\\time_bound_utilities.py:11\u001b[0m, in \u001b[0;36mtimeout\u001b[1;34m(time)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m@contextmanager\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtimeout\u001b[39m(time):\n\u001b[1;32m---> 11\u001b[0m     \u001b[39massert\u001b[39;00m is_unix_system(), \u001b[39m\"\u001b[39m\u001b[39mtimeout only supports UNIX systems for the moment as SIGALRM does not exist on Windows.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m     \u001b[39m# Register a function to raise a TimeoutError on the signal.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     signal\u001b[39m.\u001b[39msignal(signal\u001b[39m.\u001b[39mSIGALRM, raise_timeout)\n",
      "\u001b[1;31mAssertionError\u001b[0m: timeout only supports UNIX systems for the moment as SIGALRM does not exist on Windows."
     ]
    }
   ],
   "source": [
    "epsilon = 0.1\n",
    "delta = 0.1\n",
    "max_states = 1000000\n",
    "max_query_length= 1000000\n",
    "max_secs = 120\n",
    "sequence_generator = UniformLengthSequenceGenerator(alphabet, max_seq_length=100, min_seq_length=20)\n",
    "dataloader = PickleDataLoader(\"./test\")\n",
    "\n",
    "partitioner = QuantizationProbabilityPartitioner(10)\n",
    "comparator = WFAPartitionComparator(partitioner)\n",
    "teacher1  = PACBatchProbabilisticTeacher(target_model, epsilon = epsilon, delta = delta, max_seq_length = None, comparator = comparator, sequence_generator=sequence_generator, compute_epsilon_star=False)\n",
    "learner = BoundedPDFAQuantizationNAryTreeLearner(partitioner, max_states, max_query_length, max_secs, generate_partial_hipothesis = False, pre_cache_queries_for_building_hipothesis = False,  check_probabilistic_hipothesis = False)\n",
    "learning_result = learner.learn(teacher1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher2  = PACBatchProbabilisticTeacher(target_model, epsilon = epsilon, delta = delta, max_seq_length = None, comparator = comparator, sequence_generator=sequence_generator, compute_epsilon_star=False, cache_from_dataloader=dataloader)\n",
    "learner2 = BoundedPDFAQuantizationNAryTreeLearner(partitioner, max_states, max_query_length, max_secs, generate_partial_hipothesis = False, pre_cache_queries_for_building_hipothesis = False,  check_probabilistic_hipothesis = False)\n",
    "learning_result = learner2.learn(teacher2)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pythautomata.automata.wheighted_automaton_definition.probabilistic_deterministic_finite_automaton.ProbabilisticDeterministicFiniteAutomaton at 0x7f90ec367370>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_result.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and submit \n",
    "This is the creation of the model needed for the submission to the competition: you just have to run this cell. It will create in your current directory an **archive** that you can then submit on the competition website.\n",
    "\n",
    "**You should NOT modify this part, just run it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfa_wrapper import MlflowPDFA\n",
    "from submit_tools_fix import save_function\n",
    "\n",
    "mlflow_dfa = MlflowPDFA(learning_result.model)\n",
    "save_function(mlflow_dfa, len(learning_result.model.alphabet), target_model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NXdKde0kt3FR",
    "eKzzh3hot9vZ",
    "BMQF46fnw1Zk"
   ],
   "name": "PFA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "122e9f251b80a4a76e7262659287020d96f7188da42b39e3d812967db6c8742d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
