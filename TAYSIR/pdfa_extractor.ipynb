{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAYSIR competition - Track 2 Example Extraction\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This is a notebook to show the structure of a code to participate to the competition.\n",
    "\n",
    "You can also check the baseline notebook (available in the same archive) for more details about the TAYSIR models and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pymodelextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version : 2.1.0.dev20230403+cpu\n",
      "MLflow version : 2.2.2\n",
      "Your python version: 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "print('PyTorch version :', torch.__version__)\n",
    "print('MLflow version :', mlflow.__version__)\n",
    "import sys\n",
    "print(\"Your python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested with:\n",
    "* Torch version: 1.11.0+cu102\n",
    "* MLFlow version: 1.25.1\n",
    "* Python version: 3.8.10 [GCC 9.4.0]\n",
    "\n",
    "Python versions starting at 3.7 are supposed to work (but have not been tested).\n",
    "## Choosing the phase\n",
    "\n",
    "First you must select one of the phases/datasets we provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/27 19:15:16 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.1.0.dev20230403+cpu'\n",
      "2023/04/27 19:15:16 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.1.0.dev20230403+cpu'\n",
      "2023/04/27 19:15:16 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.1.0.dev20230403+cpu'\n",
      "2023/04/27 19:15:16 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.1.0.dev20230403+cpu'\n",
      "2023/04/27 19:15:16 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.1.0.dev20230403+cpu'\n",
      "2023/04/27 19:15:16 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.1.0.dev20230403+cpu'\n",
      "2023/04/27 19:15:16 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.1.0.dev20230403+cpu'\n",
      "2023/04/27 19:15:16 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.1.0.dev20230403+cpu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model: 1\n",
      "TNetwork(\n",
      "  23, 22, n_layers=2, neurons_per_layer=64, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): RNN(22, 64, batch_first=True)\n",
      "  (mach[1]): RNN(64, 64, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=22, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is RNN\n",
      "\n",
      "\n",
      "Model: 2\n",
      "TNetwork(\n",
      "  10, 9, n_layers=2, neurons_per_layer=256, cell_type=lstmx.LSTMx, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): LSTMx(\n",
      "    9, 256, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(9, 256)\n",
      "  )\n",
      "  (mach[1]): LSTMx(\n",
      "    256, 256, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(256, 256)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=9, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 9 symbols.\n",
      "The type of the recurrent cells is LSTMx\n",
      "\n",
      "\n",
      "Model: 3\n",
      "TNetwork(\n",
      "  18, 17, n_layers=2, neurons_per_layer=64, cell_type=lstmx.LSTMx, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): LSTMx(\n",
      "    17, 64, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(17, 64)\n",
      "  )\n",
      "  (mach[1]): LSTMx(\n",
      "    64, 64, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(64, 64)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=17, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 17 symbols.\n",
      "The type of the recurrent cells is LSTMx\n",
      "\n",
      "\n",
      "Model: 4\n",
      "TNetwork(\n",
      "  23, 22, cell_type=torch.nn.modules.rnn.GRU, batch_size=128, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): GRU(22, 32, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=22, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is GRU\n",
      "\n",
      "\n",
      "Model: 5\n",
      "TNetwork(\n",
      "  36, 35, neurons_per_layer=512, cell_type=lstmx.LSTMx, batch_size=64, patience=20, split_dense=True, task=lm\n",
      "  (mach[0]): LSTMx(\n",
      "    35, 512, batch_first=True\n",
      "    (drop_layer): Dropout(p=0, inplace=False)\n",
      "    (forward_layers[0]): LSTMCell(35, 512)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=35, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 35 symbols.\n",
      "The type of the recurrent cells is LSTMx\n",
      "\n",
      "\n",
      "Model: 6\n",
      "TNetwork(\n",
      "  23, 22, cell_type=torch.nn.modules.rnn.GRU, batch_size=64, patience=5, bidirectional=True, task=lm\n",
      "  (mach[0]): GRU(22, 32, batch_first=True, bidirectional=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=22, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is GRU\n",
      "\n",
      "\n",
      "Model: 7\n",
      "TNetwork(\n",
      "  23, 22, n_layers=2, neurons_per_layer=64, batch_size=64, patience=5, split_dense=True, task=lm\n",
      "  (mach[0]): RNN(22, 64, batch_first=True)\n",
      "  (mach[1]): RNN(64, 64, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=22, bias=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is RNN\n",
      "\n",
      "\n",
      "Model: 8\n",
      "TNetwork(\n",
      "  27, 26, n_layers=2, neurons_per_layer=124, batch_size=64, patience=5, task=lm\n",
      "  (mach[0]): RNN(26, 124, batch_first=True)\n",
      "  (mach[1]): RNN(124, 124, batch_first=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=124, out_features=26, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 26 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/27 19:15:16 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.1.0.dev20230403+cpu'\n",
      "2023/04/27 19:15:16 WARNING mlflow.pytorch: Stored model version '1.11.0+cu102' does not match installed PyTorch version '2.1.0.dev20230403+cpu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbols.\n",
      "The type of the recurrent cells is RNN\n",
      "\n",
      "\n",
      "Model: 9\n",
      "TNetwork(\n",
      "  23, 22, cell_type=torch.nn.modules.rnn.GRU, batch_size=64, patience=5, bidirectional=True, task=lm\n",
      "  (mach[0]): GRU(22, 32, batch_first=True, bidirectional=True)\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=22, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Softmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is GRU\n",
      "\n",
      "\n",
      "Model: 10\n",
      "DistilBertForTokenClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(64, 256, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 256)\n",
      "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-7): 8 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (lin2): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=256, out_features=64, bias=True)\n",
      ")\n",
      "The alphabet contains 35 symbols.\n",
      "The model is a transformer (DistilBertForSequenceClassification)\n"
     ]
    }
   ],
   "source": [
    "dataset_amount = 10\n",
    "for ds in range(1,dataset_amount+1):\n",
    "    DATASET = ds\n",
    "    model_name = f\"models/2.{DATASET}.taysir.model\"\n",
    "    model = mlflow.pytorch.load_model(model_name)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Model:\", ds)\n",
    "    print(model.eval())\n",
    "    try:#RNN\n",
    "        nb_letters = model.input_size -1\n",
    "        cell_type = model.cell_type\n",
    "\n",
    "        print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "        print(\"The type of the recurrent cells is\", cell_type.__name__)\n",
    "    except:\n",
    "        nb_letters = model.distilbert.config.vocab_size\n",
    "        print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "        print(\"The model is a transformer (DistilBertForSequenceClassification)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = 2 #always for this track\n",
    "DATASET = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/27 19:15:17 WARNING mlflow.pytorch: Stored model version '1.13.1+cpu' does not match installed PyTorch version '2.1.0.dev20230403+cpu'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TNetwork(\n",
       "  23, 22, n_layers=2, neurons_per_layer=64, batch_size=64, patience=5, split_dense=True, task=lm\n",
       "  (mach[0]): RNN(22, 64, batch_first=True)\n",
       "  (mach[1]): RNN(64, 64, batch_first=True)\n",
       "  (dense): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): Linear(in_features=32, out_features=22, bias=True)\n",
       "    (2): Sigmoid()\n",
       "    (3): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = f\"models/{TRACK}.{DATASET}.taysir.model\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alphabet contains 22 symbols.\n",
      "The type of the recurrent cells is RNN\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(model, 'distilbert'):#RNN\n",
    "    nb_letters = model.input_size -1\n",
    "    cell_type = model.cell_type\n",
    "\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The type of the recurrent cells is\", cell_type.__name__)\n",
    "else:\n",
    "    nb_letters = model.distilbert.config.vocab_size\n",
    "    print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "    print(\"The model is a transformer (DistilBertForSequenceClassification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The input data is in the following format :\n",
    "\n",
    "```\n",
    "[Number of sequences] [Alphabet size]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "...\n",
    "[Length of sequence] [List of symbols]\n",
    "```\n",
    "\n",
    "For example the following data :\n",
    "\n",
    "```\n",
    "5 10\n",
    "6 8 6 5 1 6 7 4 9\n",
    "12 8 6 9 4 6 8 2 1 0 6 5 9\n",
    "7 8 9 4 3 0 4 9\n",
    "4 8 0 4 9\n",
    "8 8 1 5 2 6 0 5 3 9\n",
    "```\n",
    "\n",
    "is composed of 5 sequences and has an alphabet size of 10 (so symbols are between 0 and 9) and the first sequence is composed of 6 symbols (8 6 5 1 6 7 4 9), notice that 8 is the start symbol and 9 is the end symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.base_types.alphabet import Alphabet\n",
    "\n",
    "file = f\"datasets/{TRACK}.{DATASET}.taysir.valid.words\"\n",
    "\n",
    "alphabet = None\n",
    "sequences = []\n",
    "\n",
    "#In the competition the empty sequence is defined as [alphabet_size - 2, alphabet size -1]\n",
    "#For example with the alphabet of size 22 the empty sequence is [20, 21]\n",
    "empty_sequence_len = 2\n",
    "\n",
    "with open(file) as f:\n",
    "    a = f.readline() #Skip first line (number of sequences, alphabet size)\n",
    "    headline = a.split(' ')\n",
    "    alphabet_size = int(headline[1].strip())\n",
    "    alphabet = Alphabet.from_strings([str(x) for x in range(alphabet_size - empty_sequence_len)])\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        seq = line.split(' ')\n",
    "        seq = [int(i) for i in seq[1:]] #Remove first value (length of sequence) and cast to int\n",
    "        sequences.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *sequences* is thus **a list of lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 9090\n",
      "10 first sequences:\n",
      "[20, 13, 14, 6, 0, 15, 4, 3, 5, 12, 13, 13, 14, 4, 12, 17, 21]\n",
      "[20, 3, 13, 3, 16, 6, 4, 13, 1, 21]\n",
      "[20, 13, 6, 15, 21]\n",
      "[20, 13, 10, 3, 21]\n",
      "[20, 13, 10, 3, 16, 6, 4, 13, 13, 12, 17, 4, 13, 14, 10, 0, 10, 13, 14, 4, 15, 12, 17, 21]\n",
      "[20, 3, 5, 0, 1, 4, 13, 6, 14, 4, 14, 4, 14, 13, 10, 12, 1, 5, 10, 3, 14, 5, 12, 14, 1, 12, 11, 12, 17, 18, 8, 21]\n",
      "[20, 3, 13, 3, 19, 1, 4, 3, 5, 10, 3, 19, 8, 21]\n",
      "[20, 13, 0, 1, 3, 1, 13, 3, 16, 6, 4, 13, 1, 12, 8, 0, 5, 10, 14, 12, 10, 3, 14, 1, 21]\n",
      "[20, 13, 14, 14, 6, 3, 21]\n",
      "[20, 13, 12, 13, 3, 16, 3, 16, 21]\n"
     ]
    }
   ],
   "source": [
    "print('Number of sequences:', len(sequences))\n",
    "print('10 first sequences:')\n",
    "for i in range(10):\n",
    "    print(sequences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model extraction\n",
    "\n",
    "This is where you will extract your simple own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.model_comparators.wfa_partition_comparison_strategy import WFAPartitionComparator\n",
    "from pythautomata.utilities.probability_partitioner import QuantizationProbabilityPartitioner\n",
    "#from pythautomata.model_exporters.wfa_image_exporter_with_partition_mapper import WFAImageExporterWithPartitionMapper\n",
    "from pythautomata.base_types.symbol import SymbolStr\n",
    "from pythautomata.utilities.uniform_length_sequence_generator import UniformLengthSequenceGenerator\n",
    "\n",
    "from pymodelextractor.learners.observation_tree_learners.bounded_pdfa_quantization_n_ary_tree_learner import BoundedPDFAQuantizationNAryTreeLearner\n",
    "from pymodelextractor.teachers.pac_batch_probabilistic_teacher import PACBatchProbabilisticTeacher\n",
    "from pymodelextractor.teachers.pac_probabilistic_teacher import PACProbabilisticTeacher\n",
    "from pymodelextractor.utils.pickle_data_loader import PickleDataLoader\n",
    "\n",
    "from utils import predict\n",
    "from pytorch_language_model import PytorchLanguageModel\n",
    "\n",
    "name = \"track_\" + str(TRACK) + \"_dataset_\" + str(DATASET)\n",
    "\n",
    "target_model = PytorchLanguageModel(alphabet, model, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.03931347, 0.03931397, 0.03931346, 0.10686506,\n",
       "        0.03931346, 0.03931346, 0.03931346, 0.03931346, 0.03931346,\n",
       "        0.03931346, 0.03931353, 0.03931346, 0.03931359, 0.10686506,\n",
       "        0.03931346, 0.03931346, 0.03931346, 0.03931346, 0.03931346,\n",
       "        0.03931346, 0.03931346, 0.03931346]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.full_next_symbols_probas([20,1], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(utils.full_next_symbols_probas([20,0], model)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.528450062935008e-06"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils_seq_proba = utils.sequence_probability([20,0,1,0,21], model)\n",
    "utils_seq_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.base_types.alphabet import Alphabet\n",
    "from pythautomata.base_types.symbol import SymbolStr\n",
    "from pythautomata.base_types.sequence import Sequence\n",
    "\n",
    "def get_alphabet_and_validation_sequences(ds):\n",
    "    file = f\"datasets/2.{ds}.taysir.valid.words\"\n",
    "    alphabet = None\n",
    "    sequences = []\n",
    "\n",
    "    empty_sequence_len = 2\n",
    "    with open(file) as f:\n",
    "        a = f.readline()\n",
    "        headline = a.split(' ')\n",
    "        alphabet_size = int(headline[1].strip())\n",
    "        alphabet = Alphabet.from_strings([str(x) for x in range(alphabet_size - empty_sequence_len)])\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            seq = line.split(' ')[1:]\n",
    "            seq = [SymbolStr(i) for i in seq[1:]]            \n",
    "            sequences.append(Sequence(seq))\n",
    "    return alphabet, sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pythaut_sequences = get_alphabet_and_validation_sequences(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = [target_model.terminal_symbol]\n",
    "for symbol in target_model.alphabet.symbols:\n",
    "    suffixes.append(Sequence((symbol,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_0 = Sequence([SymbolStr('0')])\n",
    "seq_010 = Sequence([SymbolStr('0'), SymbolStr('1'), SymbolStr('0')])\n",
    "seq_eps = Sequence([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.039313461631536484,\n",
       " 0.03931346535682678,\n",
       " 0.03931397199630737,\n",
       " 0.039313528686761856,\n",
       " 0.039313461631536484,\n",
       " 0.03931359201669693,\n",
       " 0.1068650633096695,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.1068650633096695,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols = list(target_model.alphabet.symbols)\n",
    "symbols.sort()\n",
    "symbols = [target_model.terminal_symbol] + symbols\n",
    "target_model.get_last_token_weights(seq_0, symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.528450062935008e-06"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.sequence_probability(seq_010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.039313461631536484,\n",
       " 0.03931346535682678,\n",
       " 0.03931397199630737,\n",
       " 0.039313528686761856,\n",
       " 0.039313461631536484,\n",
       " 0.03931359201669693,\n",
       " 0.1068650633096695,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.1068650633096695,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484,\n",
       " 0.039313461631536484]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.get_last_token_weights(seq_0, symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02963464893400669"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.sequence_probability([0,0], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.04060161, 0.04060161, 0.04060161, 0.04060161,\n",
       "       0.04060161, 0.08435925, 0.04060161, 0.04060161, 0.04060161,\n",
       "       0.04060161, 0.04432814, 0.04060161, 0.06651058, 0.05050921,\n",
       "       0.04060161, 0.04060161, 0.04060161, 0.04060161, 0.04060161,\n",
       "       0.04060161, 0.04060161, 0.06406554], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.next_symbols_probas([0,1,0], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 0, 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 2, 18, 6, 5, 12, 17, 4, 0, 13, 7, 3, 9, 16, 8, 15, 19, 10, 14, 1, 11]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.039313592,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.039313465,\n",
       "  0.10686506,\n",
       "  0.03931346,\n",
       "  0.10686506,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931346,\n",
       "  0.03931353,\n",
       "  0.03931346,\n",
       "  0.039313972,\n",
       "  0.03931346]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.get_last_token_weights_batch([seq_0], suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from last_token_weights_pickle_dataset_generator import LastTokenWeightsPickleDataSetGenerator\n",
    "#LastTokenWeightsPickleDataSetGenerator().genearte_dataset(target_model, 1000, \"./test\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pythautomata.base_types.alphabet.Alphabet'>\n"
     ]
    }
   ],
   "source": [
    "print(type(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.automata.wheighted_automaton_definition.probabilistic_deterministic_finite_automaton import \\\n",
    "    ProbabilisticDeterministicFiniteAutomaton as PDFA\n",
    "from pythautomata.automata.wheighted_automaton_definition.weighted_state import WeightedState\n",
    "from pythautomata.model_comparators.wfa_tolerance_comparison_strategy import WFAToleranceComparator\n",
    "\n",
    "from pymodelextractor.teachers.sample_batch_probabilistic_teacher import SampleBatchProbabilisticTeacher\n",
    "\n",
    "weight = 1.0/(len(alphabet)+1)\n",
    "weight = 0\n",
    "q0 = WeightedState(\"q0\", 1, weight)\n",
    "for symbol in alphabet.symbols:\n",
    "    q0.add_transition(symbol, q0, weight)\n",
    "states = {q0}\n",
    "comparator = WFAToleranceComparator()\n",
    "testPDFA = PDFA(alphabet, states, target_model.terminal_symbol, comparator, \"Test\", check_is_probabilistic = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPDFA.sequence_probability(seq_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5\n",
    "delta = 0.5\n",
    "max_states = 1000000\n",
    "max_query_length= 1000000\n",
    "max_secs = 120\n",
    "sequence_generator = UniformLengthSequenceGenerator(alphabet, max_seq_length=3, min_seq_length=2)\n",
    "dataloader = None\n",
    "#dataloader = PickleDataLoader(\"./test\")\n",
    "\n",
    "partitioner = QuantizationProbabilityPartitioner(10)\n",
    "comparator = WFAPartitionComparator(partitioner)\n",
    "#teacher = SampleBatchProbabilisticTeacher(model = target_model, comparator = comparator, sequence_generator=sequence_generator, max_seq_length=2, full_prefix_set=True,  cache_from_dataloader=dataloader)\n",
    "teacher  = PACProbabilisticTeacher(target_model, epsilon = epsilon, delta = delta, max_seq_length = None, comparator = comparator, sequence_generator=sequence_generator, compute_epsilon_star=False, cache_from_dataloader=dataloader)\n",
    "learner = BoundedPDFAQuantizationNAryTreeLearner(partitioner, max_states, max_query_length, max_secs, generate_partial_hipothesis = False, pre_cache_queries_for_building_hipothesis = False,  check_probabilistic_hipothesis = False)\n",
    "learning_result = learner.learn(teacher)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teacher2  = PACBatchProbabilisticTeacher(target_model, epsilon = epsilon, delta = delta, max_seq_length = None, comparator = comparator, sequence_generator=sequence_generator, compute_epsilon_star=False, cache_from_dataloader=dataloader)\n",
    "#learner2 = BoundedPDFAQuantizationNAryTreeLearner(partitioner, max_states, max_query_length, max_secs, generate_partial_hipothesis = False, pre_cache_queries_for_building_hipothesis = False,  check_probabilistic_hipothesis = False)\n",
    "#learning_result = learner2.learn(teacher2)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_pdfa = learning_result.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = sequence_generator.generate_words(100)\n",
    "validation_sequences = pythaut_sequences[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = metrics.compute_stats(target_model, learned_pdfa, partitioner, test_sequences, validation_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythautomata.abstract.probabilistic_model import ProbabilisticModel\n",
    "from pythautomata.base_types.sequence import Sequence\n",
    "from pythautomata.base_types.alphabet import Alphabet\n",
    "from pythautomata.base_types.symbol import SymbolStr\n",
    "\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "class QuickWrapper():\n",
    "    def __init__(self, original_model,alphabet, epsilon):\n",
    "        self._model = original_model\n",
    "        self._alphabet_len = len(alphabet) + 1\n",
    "        self._alphabet = alphabet\n",
    "        self._terminal_symbol = SymbolStr(str(self._alphabet_len))\n",
    "        self._epsilon = epsilon\n",
    "    \n",
    "    def sequence_probability(self, sequence: Sequence, debug = False):\n",
    "        adapted_sequence = self._adapt_sequence(sequence,  add_terminal=True)\n",
    "        probs = utils.full_next_symbols_probas(adapted_sequence, model)\n",
    "        probas_for_word = []\n",
    "        for i,a in enumerate(adapted_sequence):\n",
    "            probas_for_word.append(probs[i,a+1])\n",
    "        value = 1\n",
    "        for x in probas_for_word:\n",
    "            value *= x\n",
    "        return np.array(probas_for_word).prod()\n",
    "        return value        \n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self._name    \n",
    "    @property\n",
    "    def alphabet(self) -> Alphabet:\n",
    "        return self._alphabet\n",
    "    @property\n",
    "    def terminal_symbol(self):\n",
    "        return self._terminal_symbol\n",
    "    \n",
    "    def process_query(self, sequence):        \n",
    "        adapted_sequence = self._adapt_sequence(sequence, add_terminal=len(sequence) == 0)       \n",
    "        if len(sequence)==0:\n",
    "            return utils.full_next_symbols_probas(adapted_sequence, self._model)[1]\n",
    "        else:\n",
    "            return utils.next_symbols_probas(adapted_sequence, self._model)\n",
    "    \n",
    "    def _adapt_sequence(self, sequence, add_terminal = False):\n",
    "        \"\"\"\n",
    "        Method that converts sequence to list of ints and adds the starting token to the beggining \n",
    "        and terminal token at the end depending on the variable 'add_terminal'\n",
    "        \"\"\"\n",
    "        adapted_seq = [self._alphabet_len-1]\n",
    "        for symbol in sequence.value:\n",
    "            adapted_seq.append(int(symbol.value))\n",
    "\n",
    "        if add_terminal:\n",
    "            adapted_seq.append(self._alphabet_len)\n",
    "        \n",
    "        return adapted_seq\n",
    "    \n",
    "    def raw_next_symbol_probas(self, sequence: Sequence):\n",
    "        result = self.process_query(sequence)        \n",
    "        return result \n",
    "\n",
    "    def _get_symbol_index(self, symbol):\n",
    "        return int(symbol.value)+1\n",
    "\n",
    "    def next_symbol_probas(self, sequence: Sequence):\n",
    "        \"\"\"\n",
    "        Function that returns a dictionary with the probability of next symbols (not including padding_symbol)\n",
    "        Quickly implemented, depends on raw_next_symbol_probas(sequence) \n",
    "        \"\"\"                \n",
    "        next_probas = self.raw_next_symbol_probas(sequence)\n",
    "\n",
    "        symbols = list(self.alphabet.symbols) + [self._terminal_symbol]\n",
    "        intermediate_dict = {}\n",
    "        probas = np.zeros(len(symbols))\n",
    "        for idx, symbol in enumerate(symbols):\n",
    "            proba = next_probas[self._get_symbol_index(symbol)]\n",
    "            intermediate_dict[symbol] = (proba, idx)\n",
    "            probas[idx] = proba       \n",
    "\n",
    "        dict_result = {}\n",
    "        for symbol in intermediate_dict.keys():\n",
    "            dict_result[symbol] = probas[intermediate_dict[symbol][1]]\n",
    "\n",
    "        return dict_result      \n",
    "    \n",
    "    def last_token_probabilities_batch(self, sequences: list[Sequence], required_suffixes: list[Sequence]) -> list[list[float]]:\n",
    "        return self.get_last_token_weights_batch(sequences, required_suffixes)\n",
    "\n",
    "    \n",
    "    def get_last_token_weights(self, sequence, required_suffixes):\n",
    "        weights = list()\n",
    "        alphabet_symbols_weights = self.next_symbol_probas(sequence)\n",
    "        alphabet_symbols_weights = {Sequence() + k: alphabet_symbols_weights[k] for k in alphabet_symbols_weights.keys()}\n",
    "        for suffix in required_suffixes:\n",
    "            if suffix in alphabet_symbols_weights:\n",
    "                weights.append(alphabet_symbols_weights[suffix])\n",
    "            else:\n",
    "                new_sequence = sequence + suffix\n",
    "                new_prefix = Sequence(new_sequence[:-1])\n",
    "                new_suffix = new_sequence[-1]\n",
    "                next_symbol_weights = self.next_symbol_probas(new_prefix)\n",
    "                weights.append(next_symbol_weights[new_suffix])\n",
    "        return weights\n",
    "    \n",
    "    def get_last_token_weights_batch(self, sequences, required_suffixes):\n",
    "        seqs_to_query = set()\n",
    "        symbols = list(self.alphabet.symbols) + [self._terminal_symbol]\n",
    "        for seq in sequences:\n",
    "            for required_suffix in required_suffixes:\n",
    "                if required_suffix not in symbols and len(required_suffix)>1:\n",
    "                    seqs_to_query.add(seq+required_suffix[:-1])\n",
    "                else:\n",
    "                    seqs_to_query.add(seq)\n",
    "\n",
    "        result_dict = self.raw_eval_batch(list(seqs_to_query))\n",
    "        #result_dict = dict(zip(seqs_to_query, query_results))\n",
    "        results = []\n",
    "        for seq in sequences:\n",
    "            seq_result = []\n",
    "            for required_suffix in required_suffixes:\n",
    "                if required_suffix not in symbols and len(required_suffix)>1:\n",
    "                    seq_result.append(result_dict[seq+required_suffix[:-1]][required_suffix[-1]])\n",
    "                else:\n",
    "                    if required_suffix not in symbols:\n",
    "                        required_suffix = SymbolStr(required_suffix.value[0].value)\n",
    "                    seq_result.append(result_dict[seq][self._get_symbol_index(required_suffix)])\n",
    "            results.append(seq_result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def raw_eval_batch(self, sequences: List[Sequence]):\n",
    "        if not hasattr(self, '_model'):\n",
    "            raise AttributeError\n",
    "\n",
    "        sequences_by_length = defaultdict(lambda: [])\n",
    "        for seq in sequences:\n",
    "            sequences_by_length[len(seq)].append(seq)            \n",
    "        query_results = []\n",
    "        seqs_to_query = []\n",
    "        for length in sequences_by_length:            \n",
    "            seqs =  sequences_by_length[length]\n",
    "            adapted_sequences = list(map(lambda x: self._adapt_sequence(x), seqs))     \n",
    "            adapted_sequences_np = np.asarray(adapted_sequences)\n",
    "\n",
    "            #if length == 1:\n",
    "            #    adapted_sequences_np = adapted_sequences_np.reshape((-1, 1, len(adapted_sequences_np[0]))) \n",
    "            if length == 0:                \n",
    "                seq = Sequence()\n",
    "                #adapted_sequence = self._adapt_sequence(seqs[0], add_terminal=True)    \n",
    "                #adapted_sequence_np = np.asarray(adapted_sequence)\n",
    "                #result = utils.full_next_symbols_probas(adapted_sequence_np, self._model)\n",
    "                #model_evaluation = [result[0]]\n",
    "                result = self.process_query(seq)\n",
    "                model_evaluation =[result]\n",
    "            else:\n",
    "                model_evaluation = utils.full_next_symbols_probas_batch(adapted_sequences_np, self._model)[:,-1]\n",
    "            seqs_to_query.extend(seqs)\n",
    "            query_results.extend(model_evaluation)\n",
    "\n",
    "        result_dict = dict(zip(seqs_to_query, query_results))            \n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b = QuickWrapper(model, alphabet, 0.0)\n",
    "model_b._epsilon == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = metrics.compute_stats(target_model, model_b, partitioner, test_sequences, validation_sequences)\n",
    "model_b._epsilon == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test Accuracy': 1.0, 'Test Taysir MSE': 0.0, 'Validation Taysir MSE': 0.0}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = target_model.sequence_probability(seq_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model_b.sequence_probability(seq_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a-b)*10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test Accuracy': 1.0,\n",
       " 'Test Taysir MSE': 1.5372453982012312e-17,\n",
       " 'Validation Taysir MSE': 1.4419992677867512e-21}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test Accuracy': 0.61,\n",
       " 'Test Taysir MSE': 2.6053120983876092,\n",
       " 'Validation Taysir MSE': 8.057360028618274e-06}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and submit \n",
    "This is the creation of the model needed for the submission to the competition: you just have to run this cell. It will create in your current directory an **archive** that you can then submit on the competition website.\n",
    "\n",
    "**You should NOT modify this part, just run it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_pdfa_wrapper import MlflowFastPDFA\n",
    "from submit_tools_fix import save_function\n",
    "from fast_pdfa_converter import FastProbabilisticDeterministicFiniteAutomatonConverter as FastPDFAConverter\n",
    "\n",
    "fast_pdfa = FastPDFAConverter().to_fast_pdfa(learning_result.model)\n",
    "mlflow_fast_pdfa = MlflowFastPDFA(fast_pdfa)\n",
    "save_function(mlflow_fast_pdfa, len(learning_result.model.alphabet), target_model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_pdfa_wrapper import MlflowFastPDFA\n",
    "from submit_tools_fix import save_function\n",
    "from fast_pdfa_converter import FastProbabilisticDeterministicFiniteAutomatonConverter as FastPDFAConverter\n",
    "\n",
    "fast_pdfa = FastPDFAConverter().to_fast_pdfa(testPDFA)\n",
    "mlflow_fast_pdfa = MlflowFastPDFA(fast_pdfa)\n",
    "save_function(mlflow_fast_pdfa, len(testPDFA.alphabet), \"TEST_PDFA_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_function(mlflow_fast_pdfa, len(testPDFA.alphabet), \"TEST_PDFA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip_path = f\"predicted_models/{target_model.name}.zip\"\n",
    "zip_path = f\"predicted_models/TEST_PDFA.zip\"\n",
    "from load_func import load_function\n",
    "print(sequences[0:10])\n",
    "load_function(zip_path, sequences[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NXdKde0kt3FR",
    "eKzzh3hot9vZ",
    "BMQF46fnw1Zk"
   ],
   "name": "PFA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "122e9f251b80a4a76e7262659287020d96f7188da42b39e3d812967db6c8742d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
